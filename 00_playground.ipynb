{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM (Gemma-2B-it)\n",
    "Documentation:\n",
    "- [Gemma-2B-it](https://huggingface.co/google/gemma-2b-it)\n",
    "- [AutoTokenizer](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer)\n",
    "  - [GemmaTokenizer](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/gemma#transformers.GemmaTokenizer)\n",
    "  - [Tokenizer call](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__)\n",
    "- [AutoModelForCausalLM](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoModelForCausalLM)\n",
    "  - [GemmaModel](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/gemma#transformers.GemmaModel)\n",
    "  - [Model generation](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for LLM\n",
    "llm_model_id = \"google/gemma-2b-it\"\n",
    "llm_model_folder = f\"./models/{llm_model_id.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model 'google/gemma-2b-it' to './models/gemma-2b-it'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1c8a294a094b0f9c77e0c4d4dbe602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776f21105159463d9a28f38e59f81c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma-2b-it.gguf:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/isaacperezborrero/Documents/vlm-gemma2b/models/gemma-2b-it'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# First, go to the model's web site and apply to access the model (https://huggingface.co/google/gemma-2b-it). Once you \n",
    "# are granted access, use huggingface-cli for authentication (https://huggingface.co/docs/huggingface_hub/guides/cli)\n",
    "\n",
    "# Speed up file transfers with the Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# Download the model\n",
    "print(f\"Downloading model '{llm_model_id}' to '{llm_model_folder}'\")\n",
    "snapshot_download(llm_model_id, local_dir=llm_model_folder, local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3b9ae19ed945b8b9ec534e7d1263b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_folder, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Tokenizer\n",
    "The definition of tokenization, as given by Stanford NLP group is:\n",
    "\n",
    "_Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation_\n",
    "\n",
    "Tokenizer used in `Gemma-2B-it` is based on [byte-level Byte-Pair-Encoding](https://github.com/huggingface/transformers/blob/092f1fdaa4224fdd88c616dc9678e6fcb37bfffd/src/transformers/models/gemma/tokenization_gemma.py#L39). Each string is split according this algorithm and each part is assigned to a token.\n",
    "\n",
    "A token is represented as an embedding vector internally in the model. During training, the model learns the values for the embedding.\n",
    "\n",
    "Each token has an identifier that allows to map the string to the corresponding embedding.\n",
    "\n",
    "The vocabulary (number of tokens) of `Gemma-2B-it` is `256000` tokens and each token uses a `2048`-dimensional embedding vector.\n",
    "\n",
    "We can transform strings into tokens (represented as integers) and the other way around. This can be achieved with the\n",
    "methods:\n",
    "  - [`encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode): `str` -> `list[int, ...]`\n",
    "  - [`decode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode): `list[int, ...]` -> `str`\n",
    "\n",
    "The model uses some special tokens to indicate that it is the beginning of a sentence or that it ends the user prompt among other uses. These are the special tokens used by `Gemma-2B-it`:\n",
    "  - `<bos>`: Stands for \"beginning of sentence.\" This token is used to indicate the start of a sentence or text sequence. It signals the model to start generating or processing text.\n",
    "  - `<eos>`: Stands for \"end of sentence.\" This token is used to signify the end of a sentence or text sequence. It helps the model determine when to stop generating text or when a complete thought has been processed.\n",
    "  - `<pad>`: Stands for \"padding.\" This token is used to fill in blank spaces in text sequences to ensure they all have the same length when processed in batches. Padding is necessary because many machine learning models require input data of a consistent size.\n",
    "  - `<unk>`: Stands for \"unknown.\" This token is used to represent words or characters that are not found in the model's vocabulary. It acts as a placeholder for any unrecognized or out-of-vocabulary elements.\n",
    "  - `<start_of_turn>`: This token is used to indicate the beginning of a speaker's turn or a new segment of conversation.\n",
    "  - `<end_of_turn>`: Similar to `<start_of_turn>`, this token indicates the end of a speaker's turn or conversation segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of 256000 tokens\n",
      "Special tokens: ['<bos>', '<eos>', '<unk>', '<pad>', '<start_of_turn>', '<end_of_turn>']\n",
      "Mapping of some tokens:\n",
      "Token '<bos>' correspond to id '2'\n",
      "Token '<eos>' correspond to id '1'\n",
      "Token '<unk>' correspond to id '3'\n",
      "Token '<pad>' correspond to id '0'\n",
      "Token '<start_of_turn>' correspond to id '106'\n",
      "Token '<end_of_turn>' correspond to id '107'\n",
      "Token '0' correspond to id '235276'\n",
      "Token '1' correspond to id '235274'\n",
      "Token '2' correspond to id '235284'\n",
      "Token 'a' correspond to id '235250'\n",
      "Token 'b' correspond to id '235268'\n",
      "Token 'c' correspond to id '235260'\n",
      "Token 'the' correspond to id '1175'\n",
      "Token '\\n\\n' correspond to id '109'\n",
      "Encoding:\n",
      "'the0a1b' is encoded as [2, 1175, 235276, 235250, 235274, 235268]\n",
      "[2, 1175, 235276, 235250, 235274, 235268] is decoded as '<bos>the0a1b'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using a vocabulary of {llm_tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Special tokens (some of them already exists on HuggingFace library and others are new from Gemma)\n",
    "hf_special_tokens = [llm_tokenizer.bos_token, llm_tokenizer.eos_token, llm_tokenizer.unk_token, llm_tokenizer.pad_token]\n",
    "gemma_special_tokens = llm_tokenizer.additional_special_tokens\n",
    "special_tokens = hf_special_tokens + gemma_special_tokens\n",
    "print(f\"Special tokens: {special_tokens}\")\n",
    "\n",
    "# Print some tokens and their ids\n",
    "vocab = llm_tokenizer.get_vocab()\n",
    "print(\"Mapping of some tokens:\")\n",
    "for key in special_tokens + ['0', '1', '2', 'a', 'b', 'c', 'the', '\\n\\n']:\n",
    "    print(f\"Token {repr(key)} correspond to id '{vocab[key]}'\")  # also valid: llm_tokenizer.token_to_id(key)\n",
    "\n",
    "# Let's encode a string\n",
    "text = \"the0a1b\"\n",
    "text_encoded = llm_tokenizer.encode(text)\n",
    "\n",
    "# It adds the <bos> token at the begining. To disable it, pass add_special_tokens=False to encode method.\n",
    "print(\"Encoding:\")\n",
    "print(f\"'{text}' is encoded as {text_encoded}\")\n",
    "\n",
    "# Decode the tokens back to a string\n",
    "text_decoded = llm_tokenizer.decode(text_encoded)\n",
    "print(f\"{text_encoded} is decoded as '{text_decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need the id of the tokens so the model can use its embedding and the __attention mask__. The primary use of the attention mask is to allow the model to differentiate between the actual content and padding within the input sequences. \n",
    "\n",
    "In NLP tasks, input sequences can vary in length. However, most neural networks require inputs to be of a fixed size. To address this, sequences are often padded with special tokens to reach a uniform length before being fed into a model. While necessary for processing, these padding tokens should not influence the model's predictions. The attention mask tells the model which parts of the input are actual data and which are padding. The attention mask is a binary mask (i.e., consisting of zeros and ones) indicating which tokens in the sequence are padding tokens and which are not. For most models, a 1 indicates a real token and a 0 indicates a padding token. During the attention calculation in the model, the mask is used to virtually eliminate the effect of padding tokens by setting their attention scores to a very large negative value (making their resulting softmax scores close to zero). This way, when the softmax function is applied to the attention scores, the padding tokens do not contribute to the final output.\n",
    "\n",
    "Consider a scenario where you have two sentences:\n",
    "\n",
    "- Sentence A: `\"Hello, how are you?\"`\n",
    "- Sentence B: `\"Good morning.\"`\n",
    "\n",
    "If you need to pad these sentences to a fixed length of 5 tokens each, your input might look like this after tokenization and padding (assuming `[PAD]` is the padding token):\n",
    "\n",
    "- Sentence A Tokens: `[Hello, how, are, you, ?]`\n",
    "- Sentence B Tokens: `[Good, morning, [PAD], [PAD], [PAD]]`\n",
    "\n",
    "Correspondingly, the attention mask for these inputs would be:\n",
    "\n",
    "- Sentence A Mask: `[1, 1, 1, 1, 1]` (indicating all tokens should be attended to)\n",
    "- Sentence B Mask: `[1, 1, 0, 0, 0]` (indicating only the first two tokens are real and the rest are padding)\n",
    "\n",
    "By using the attention mask, the model knows to focus on the meaningful content and ignore the padding, thus ensuring more accurate processing and analysis of the input data.\n",
    "\n",
    "We can prepare everything we need to train the model using the Tokenizer call method (`__call__`). The `__call__` method is a more flexible and feature-rich interface to the tokenizer. When you use the tokenizer object like a function (i.e., tokenizer(\"Your input text here.\")), you're actually invoking its `__call__` method. This method can perform tokenization (similar to `encode`), but it also handles additional features like padding the input to a fixed length, truncating inputs to the model's maximum length, returning tensors ready to feed into a model, and more. Essentially, it's designed to prepare the model inputs in one step.\n",
    "\n",
    "The `__call__` method often returns a dictionary containing various keys such as `input_ids`, `token_type_ids`, and `attention_mask`, depending on the configuration and the needs of the specific model you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer output for two sentences: ['the', 'the0a1b']\n",
      "input_ids (2, 6): tensor([[     0,      0,      0,      0,      2,   1175],\n",
      "        [     2,   1175, 235276, 235250, 235274, 235268]])\n",
      "attention_mask (2, 6): tensor([[0, 0, 0, 0, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Since we plan to use the tokenizer to train the model and the model only accepts Pytorch Tensors as input, we need to\n",
    "# set return_tensors=\"pt\". We also need to add padding because the sentences are not the same length.\n",
    "sentences = [\"the\", \"the0a1b\"]\n",
    "tokenizer_output = llm_tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(f\"Tokenizer output for two sentences: {sentences}\")\n",
    "for k, v in tokenizer_output.items():\n",
    "    print(f\"{k} {tuple(v.shape)}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemma-2B-it is an instruction model. When a language model (LLM) is described as an \"instruction model,\" it refers to a type of language model that is specifically trained or fine-tuned to follow or understand natural language instructions and generate responses or outputs based on those instructions. \n",
    "\n",
    "Instruction models are typically created by training or fine-tuning large language models on datasets that contain pairs of instructions and corresponding outputs or actions. \n",
    "\n",
    "Beyond serving as instruction models, LLMs are increasingly employed for facilitating chat-based interactions. In such scenarios, rather than generating continuations for a singular text string, these models engage in conversations. These interactions are structured as sequences of messages, each tagged with a role—such as \"user\" or \"assistant\"—and accompanied by the corresponding message text. This approach enables the model to understand and maintain the flow of dialogue, distinguishing between different speakers and their messages.\n",
    "\n",
    "Much like tokenization, different models expect very different input formats for chat. This is the reason HuggingFace added __chat templates__ as a feature. Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects.\n",
    "\n",
    "Chat template are written using [Jinja templates](https://jinja.palletsprojects.com/en/3.1.x/templates/).\n",
    "\n",
    "__Gemma-2B-it is not a chat model__ but Huggingface sets a default chat template to each model so that we can inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}\n",
      "{% if messages[0]['role'] == 'system' %}\n",
      "{{ raise_exception('System role not supported') }}\n",
      "{% endif %}\n",
      "{% for message in messages %}\n",
      "{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
      "{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "{% endif %}\n",
      "{% if (message['role'] == 'assistant') %}\n",
      "{% set role = 'model' %}\n",
      "{% else %}\n",
      "{% set role = message['role'] %}\n",
      "{% endif %}\n",
      "{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}\n",
      "{% endfor %}\n",
      "{% if add_generation_prompt %}\n",
      "{{'<start_of_turn>model\n",
      "'}}\n",
      "{% endif %}\n",
      "Chat template applied:\n",
      "<bos><start_of_turn>user\n",
      "Hello, how are you?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "I'm doing great. How can I help you today?<end_of_turn>\n",
      "<start_of_turn>user\n",
      "I'd like to show off how chat templating works!<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The chat template for a model is stored on the tokenizer.chat_template attribute\n",
    "chat_template = llm_tokenizer.chat_template\n",
    "print(\"}\\n{\".join(chat_template.split(\"}{\")))\n",
    "\n",
    "# We can apply the template to any chat. A chat consists of a list of JSONs with the role and the message content\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "\n",
    "]\n",
    "\n",
    "print(f\"Chat template applied:\\n{llm_tokenizer.apply_chat_template(chat, tokenize=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have not used embeddings. We have converted the tokens (text) to their identifiers with the `encode` method and then converted them back to text with the `decode` method. When you use __call__ method does the same as `encode`, it just has some extra functionality. \n",
    "\n",
    "To obtain the embedding of a token you typically don't use the Tokenizer class directly for embeddings. Instead, the Tokenizer class is used to convert text to tokens or token IDs, which are then fed into a model to obtain embeddings. __The embeddings themselves are retrieved from the model, not the tokenizer__.\n",
    "\n",
    "In the case of `Gemma-2B-it`, the embedding model is stored at `model.embed_tokens`. It's a Pytorch `nn.Embedding` layer, which is a simple lookup table that stores embeddings of a fixed dictionary and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'the' token (id 1175) with shape torch.Size([1, 2048]): tensor([[ 0.1924, -0.0713, -0.0732,  ...,  0.0156, -0.0017,  0.0288]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Embedding for 'the' token\n",
    "embedding = llm_model.model.embed_tokens(torch.LongTensor([1175]))\n",
    "print(f\"Embedding for 'the' token (id 1175) with shape {embedding.shape}: {embedding.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add new tokens to the tokenizer without training it again.\n",
    "\n",
    "The new token we want to add can be a special token. They play a critical role in signaling specific types of information or instructions to the model. They help the model understand the structure of the input data, differentiate between various segments of data, and perform specific actions based on the type of token encountered.\n",
    "\n",
    "For example, Padding tokens (`[PAD]`) are used to ensure that all input sequences in a batch have the same length. They are ignored by the model during processing, allowing sequences of varying lengths to be batched together without affecting the model's performance.\n",
    "\n",
    "When we add a new token we have to modify the Tokenizer and the embedding layer of the LLM. First we have to add the new token to the Tokenizer so that it can be identified, and then we have to modify the embedding layer to add a new random embedding for this new token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id for 'new_token' token before: [1404, 235298, 5526]\n",
      "token id for 'new_token' token after: [256000]\n",
      "Embedding for 'the' and 'new_token' tokens (id 1175 and 256000) with shape torch.Size([2, 2048]):tensor([[ 0.1924, -0.0713, -0.0732,  ...,  0.0156, -0.0017,  0.0288],\n",
      "        [ 0.0056, -0.0140, -0.0007,  ..., -0.0064, -0.0337, -0.0594]])\n",
      "token id for '<image>' token: [2, 256001]\n",
      "Tokenizer output for sentence: <image> What do you see in this picture?\n",
      "input_ids (1, 10): tensor([[     2, 256001,   2439,    749,    692,   1443,    575,    736,   5642,\n",
      "         235336]])\n",
      "attention_mask (1, 10): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# new tokens \n",
    "new_tokens = [\"new_token\"]\n",
    "\n",
    "# check if the tokens are already in the vocabulary\n",
    "assert len(set(new_tokens) - set(llm_tokenizer.vocab.keys())) > 0\n",
    "\n",
    "# add the tokens to the tokenizer vocabulary\n",
    "print(f\"token id for '{new_tokens[0]}' token before: {llm_tokenizer.encode(new_tokens[0], add_special_tokens=False)}\")\n",
    "llm_tokenizer.add_tokens(list(new_tokens))\n",
    "print(f\"token id for '{new_tokens[0]}' token after: {llm_tokenizer.encode(new_tokens[0], add_special_tokens=False)}\")\n",
    "\n",
    "# add new, random embeddings for the new tokens\n",
    "llm_model.resize_token_embeddings(len(llm_tokenizer))\n",
    "\n",
    "# The old embeddings are the same, but we have a new one for the new token\n",
    "embedding = llm_model.model.embed_tokens(torch.LongTensor([1175, 256000]))\n",
    "print(f\"Embedding for 'the' and 'new_token' tokens (id 1175 and 256000) with shape {embedding.shape}:\" \n",
    "      f\"{embedding.detach()}\")\n",
    "\n",
    "# If we want to add a new special token, we just have to set special_token=True in the Tokenizer\n",
    "new_special_tokens = [\"<image>\"]\n",
    "llm_tokenizer.add_tokens(new_special_tokens, special_tokens=True)\n",
    "llm_model.resize_token_embeddings(len(llm_tokenizer))\n",
    "print(f\"token id for '{new_special_tokens[0]}' token: {llm_tokenizer.encode(new_special_tokens[0])}\")  # 2: <bos>\n",
    "\n",
    "sentence = \"<image> What do you see in this picture?\"\n",
    "tokenizer_output = llm_tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(f\"Tokenizer output for sentence: {sentence}\")\n",
    "for k, v in tokenizer_output.items():\n",
    "    print(f\"{k} {tuple(v.shape)}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do inference with the LLM we only need to pass the output of the tokenizer to the `generate` method of the model (`input_ids` and, optionally, `attention_mask`).\n",
    "The output of the LLM is a dictionary with the input_ids of the tokens, so we need to use the tokenizer to decode it and create the final text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model's output (id of the tokens): tensor([[     2,   1841,    603,    573,  12324,   8180,    575,    573,   2134,\n",
      "         235336,    109,  24059,  99771, 235269,   7023,    575,    573, 148783,\n",
      "         235269,    603,    573,   9393,   8180,    611,  10379, 235269,    675,\n",
      "            476,  14538,  28554,    576, 235248, 235321, 235269, 235321, 235310,\n",
      "         235321, 235265, 235321, 235318,  18678,    591, 235284, 235315, 235269,\n",
      "         235276, 235304, 235284, 235265, 235310,   5368,    846,      1]])\n",
      "Final output with special tokens: <bos>What is the biggest mountain in the world?\n",
      "\n",
      "Mount Everest, located in the Himalayas, is the highest mountain on Earth, with a peak elevation of 8,848.86 meters (29,032.4 feet).<eos>\n",
      "Final output without special tokens: What is the biggest mountain in the world?\n",
      "\n",
      "Mount Everest, located in the Himalayas, is the highest mountain on Earth, with a peak elevation of 8,848.86 meters (29,032.4 feet).\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the biggest mountain in the world?\"\n",
    "inputs = llm_tokenizer(text, return_tensors=\"pt\")\n",
    "generate_ids = llm_model.generate(**inputs, max_length=256)\n",
    "\n",
    "# The output of the model is in batch format so we use batch_decode instead of decode to get the text. Also, we set \n",
    "# skip_special_tokens=True to remove all special tokens from the output (like <bos>). The output is a list with the \n",
    "# decoded text, so we only need the first element of the list ([0]), as we only have one sentence.\n",
    "text_output_with_special_tokens = llm_tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "text_output_without_special_tokens = llm_tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(f\"Raw model's output (id of the tokens): {generate_ids}\")\n",
    "print(f\"Final output with special tokens: {text_output_with_special_tokens}\")\n",
    "print(f\"Final output without special tokens: {text_output_without_special_tokens}\")\n",
    "\n",
    "# Note that the output also contains the input text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the configuration, architecture, the number of parameters of the model and the memory it needs using the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM configuration:\n",
      "GemmaConfig {\n",
      "  \"_name_or_path\": \"./models/gemma-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "Model architecture:\n",
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n",
      "Number of parameters: 2506172416\n",
      "Memory requirement (in Bytes): 10091807744\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "print(f\"LLM configuration:\\n{llm_model.config}\")\n",
    "\n",
    "# Model architecture\n",
    "print(f\"Model architecture:\\n{llm_model}\")\n",
    "\n",
    "# Number of parameters\n",
    "print(f\"Number of parameters: {llm_model.num_parameters()}\")\n",
    "\n",
    "# Memory requirements (in Bytes)\n",
    "print(f\"Memory requirement (in Bytes) for full-precision (float32) model: {llm_model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual encoder (CLIP model)\n",
    "CLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image classification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text features. Both the text and visual features are then projected to a latent space with identical dimension. The dot product between the projected image and text features is then used as a similar score.\n",
    "\n",
    "A distinctive aspect of CLIP's architecture is its processor, which encapsulates the mechanisms for processing both images and text. Therefore, CLIP processor has two main components:\n",
    "\n",
    "- __Image processor__: The input image undergoes a series of transformations to match the expected input format for the CLIP model. This typically includes resizing the image, adjusting its aspect ratio, and applying normalization to align with the model's training data.\n",
    "\n",
    "- __Text processor__: For processing text, CLIP utilizes a tokenizer. The tokenizer breaks down text input into tokens, which are then encoded into embeddings by the LLM. These embeddings represent the textual information in a form that can be projected into the same latent space as the image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for CLIP model\n",
    "clip_model_id = \"google/siglip-base-patch16-384\"\n",
    "clip_model_folder = f\"./models/{clip_model_id.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model 'google/siglip-base-patch16-384' to './models/siglip-base-patch16-384'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b22156f7ff4cc7b0485d157e1ff6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065d22b8f4014a4da44a01fe146e341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/814M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8301b0f05104f4aa1f4659a3e4c402c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03c7dec074b46a398b1392bf9f54e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/322 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e2eba770ca40fea052cf675d891ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8a255c484e4ff2be0854b21a5c41c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99aae4b9774b498fb8a075f29e8399f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29cc896af9d4b419b063d6a9ed8dd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb8776ab5e24fa2bd7c7beee1e80246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/isaacperezborrero/Documents/vlm-gemma2b/models/siglip-base-patch16-384'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Speed up file transfers with the Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# Download the model\n",
    "print(f\"Downloading model '{clip_model_id}' to '{clip_model_folder}'\")\n",
    "snapshot_download(clip_model_id, local_dir=clip_model_folder, local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "clip_model = AutoModel.from_pretrained(clip_model_folder, local_files_only=True)\n",
    "clip_processor = AutoProcessor.from_pretrained(clip_model_folder, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Processor\n",
    "\n",
    "The main method of the processor is the `__call__` method. It is used to prepare for the model one or several sequences(s) and image(s). We can process the image and the text in one call. \n",
    "\n",
    "The output of the processor is:\n",
    "- **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n",
    "- **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
    "    `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n",
    "    `None`).\n",
    "- **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP processor's output for two sentences and one image:\n",
      "input_ids (2, 64): tensor([[ 262,  266, 1304,  267,  454, 6473,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1],\n",
      "        [ 262,  266, 1304,  267,  454, 3014,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1]])\n",
      "pixel_values (1, 3, 384, 384): tensor([[[[ 0.1059,  0.1294,  0.1451,  ..., -0.1608, -0.2078, -0.1843],\n",
      "          [ 0.0980,  0.1216,  0.2078,  ..., -0.2157, -0.1765, -0.1922],\n",
      "          [ 0.1216,  0.1608,  0.1686,  ..., -0.1843, -0.2000, -0.2235],\n",
      "          ...,\n",
      "          [ 0.8275,  0.8353,  0.7961,  ...,  0.5216,  0.4824,  0.4510],\n",
      "          [ 0.7961,  0.8118,  0.7412,  ...,  0.1843,  0.0510, -0.0667],\n",
      "          [ 0.8824,  0.8196,  0.6941,  ..., -0.2941, -0.3804, -0.4196]],\n",
      "\n",
      "         [[-0.8039, -0.8196, -0.8431,  ..., -0.9216, -0.8902, -0.9059],\n",
      "          [-0.8039, -0.8118, -0.7725,  ..., -0.8824, -0.9059, -0.8824],\n",
      "          [-0.7725, -0.7725, -0.7882,  ..., -0.8667, -0.8745, -0.8824],\n",
      "          ...,\n",
      "          [-0.2627, -0.2549, -0.2863,  ..., -0.5059, -0.5451, -0.5451],\n",
      "          [-0.3255, -0.2863, -0.3412,  ..., -0.7098, -0.7647, -0.8510],\n",
      "          [-0.2157, -0.2784, -0.4039,  ..., -0.8588, -0.8824, -0.8667]],\n",
      "\n",
      "         [[-0.5294, -0.4745, -0.4588,  ..., -0.6941, -0.6941, -0.7255],\n",
      "          [-0.6000, -0.5765, -0.4275,  ..., -0.6863, -0.7098, -0.7333],\n",
      "          [-0.5922, -0.5216, -0.5608,  ..., -0.6863, -0.7176, -0.7647],\n",
      "          ...,\n",
      "          [ 0.5373,  0.5373,  0.4745,  ...,  0.2784,  0.2392,  0.2314],\n",
      "          [ 0.6392,  0.5843,  0.5294,  ..., -0.2000, -0.3255, -0.4275],\n",
      "          [ 0.4902,  0.5922,  0.5686,  ..., -0.6941, -0.7412, -0.6941]]]])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image \n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n",
    "clip_processor_output = clip_processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "print(f\"CLIP processor's output for two sentences and one image:\")\n",
    "for k, v in clip_processor_output.items():\n",
    "    print(f\"{k} {tuple(v.shape)}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what the processor does by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiglipProcessor:\n",
      "- image_processor: SiglipImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"SiglipImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"processor_class\": \"SiglipProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: SiglipTokenizer(name_or_path='./models/siglip-base-patch16-384', vocab_size=32000, model_max_length=64, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t1: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"SiglipProcessor\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clip_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image Processor\n",
    "If we do not want to use the text processor, we can reduce the memory usage of the model by extracting the image processor.\n",
    "\n",
    "We can use its __call__ method as with clip processor. In this case, it only returns `pixel_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image processor's output:\n",
      "pixel_values (1, 3, 384, 384): tensor([[[[ 0.1059,  0.1294,  0.1451,  ..., -0.1608, -0.2078, -0.1843],\n",
      "          [ 0.0980,  0.1216,  0.2078,  ..., -0.2157, -0.1765, -0.1922],\n",
      "          [ 0.1216,  0.1608,  0.1686,  ..., -0.1843, -0.2000, -0.2235],\n",
      "          ...,\n",
      "          [ 0.8275,  0.8353,  0.7961,  ...,  0.5216,  0.4824,  0.4510],\n",
      "          [ 0.7961,  0.8118,  0.7412,  ...,  0.1843,  0.0510, -0.0667],\n",
      "          [ 0.8824,  0.8196,  0.6941,  ..., -0.2941, -0.3804, -0.4196]],\n",
      "\n",
      "         [[-0.8039, -0.8196, -0.8431,  ..., -0.9216, -0.8902, -0.9059],\n",
      "          [-0.8039, -0.8118, -0.7725,  ..., -0.8824, -0.9059, -0.8824],\n",
      "          [-0.7725, -0.7725, -0.7882,  ..., -0.8667, -0.8745, -0.8824],\n",
      "          ...,\n",
      "          [-0.2627, -0.2549, -0.2863,  ..., -0.5059, -0.5451, -0.5451],\n",
      "          [-0.3255, -0.2863, -0.3412,  ..., -0.7098, -0.7647, -0.8510],\n",
      "          [-0.2157, -0.2784, -0.4039,  ..., -0.8588, -0.8824, -0.8667]],\n",
      "\n",
      "         [[-0.5294, -0.4745, -0.4588,  ..., -0.6941, -0.6941, -0.7255],\n",
      "          [-0.6000, -0.5765, -0.4275,  ..., -0.6863, -0.7098, -0.7333],\n",
      "          [-0.5922, -0.5216, -0.5608,  ..., -0.6863, -0.7176, -0.7647],\n",
      "          ...,\n",
      "          [ 0.5373,  0.5373,  0.4745,  ...,  0.2784,  0.2392,  0.2314],\n",
      "          [ 0.6392,  0.5843,  0.5294,  ..., -0.2000, -0.3255, -0.4275],\n",
      "          [ 0.4902,  0.5922,  0.5686,  ..., -0.6941, -0.7412, -0.6941]]]])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image \n",
    "\n",
    "# Extract the image processor\n",
    "image_processor = clip_processor.image_processor \n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image_processor_output = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Image processor's output:\")\n",
    "for k, v in image_processor_output.items():\n",
    "    print(f\"{k} {tuple(v.shape)}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiglipImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"SiglipImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"processor_class\": \"SiglipProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(image_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the CLIP Model\n",
    "The CLIP model quantifies the resemblance between images and text by first extracting their embeddings and then computing the cosine similarity between them.\n",
    "\n",
    "To extract embeddings for analysis, utilize the methods outlined below:\n",
    "- `get_text_features`: Retrieves the embedding for text.\n",
    "- `get_image_features`: Retrieves the embedding for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: (2, 768)\n",
      "Image features shape: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image \n",
    "\n",
    "# Text features\n",
    "inputs = clip_processor(text=[\"a photo of 2 cats\", \"a photo of 2 dogs\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.get_text_features(**inputs)\n",
    "print(f\"Text features shape: {tuple(text_features.shape)}\")\n",
    "\n",
    "# Image features\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = clip_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**inputs)\n",
    "print(f\"Image features shape: {tuple(image_features.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, both the image and the text embedding have the same dimensions. \n",
    "\n",
    "Now that we have the embeddings we can calculate the cosine similarity. However, we can speed up the computation because if we normalize these embeddings first, we enable the computation of cosine similarity directly through the dot product of the two embeddings:\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors, offering a way to quantify how similar these vectors are irrespective of their magnitude. It's calculated using the formula:\n",
    "\n",
    "$$ \\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\|\\|B\\|} $$\n",
    "\n",
    "where $A$ and $B$ are vectors, $A \\cdot B$ denotes the dot product of vectors $A$ and $B$, and $\\|A\\|$ and $\\|B\\|$ are the magnitudes (or norms) of vectors $A$ and $B$, respectively.\n",
    "\n",
    "The process of normalizing a vector involves dividing each component of the vector by the vector's magnitude, resulting in a unit vector (a vector of magnitude 1 in the same direction as the original vector). When vectors are normalized to unit vectors, their magnitudes ($\\|A\\|$ and $\\|B\\|$) become 1. Therefore, the denominator of the cosine similarity formula ($\\|A\\|\\|B\\|$) becomes 1.\n",
    "\n",
    "After normalization, the cosine similarity formula simplifies to just the dot product of $A$ and $B$ because the denominator is 1:\n",
    "\n",
    "$$ \\text{Cosine Similarity} = \\frac{A \\cdot B}{1} = A \\cdot B $$\n",
    "\n",
    "This simplification allows for a straightforward calculation of cosine similarity as the dot product between two normalized vectors, effectively measuring the cosine of the angle between them. This approach is particularly useful in comparing the orientation (or direction) of vectors in high-dimensional spaces, like those used for text and image embeddings, without being affected by their magnitude.\n",
    "\n",
    "The actual implementation of the cosine similarity of CLIP uses two more elements:\n",
    "\n",
    "$$ \\text{Cosine Similarity} = A \\cdot B * e^{\\text{logit\\_scale}} + \\text{logit\\_bias}$$\n",
    "\n",
    "`logit_scale` and `logit_bias` are used to adjust the scale and bias of the cosine similarity values before they are utilized in subsequent computations or as logits for classification tasks. Let's break down their purposes:\n",
    "\n",
    "  - `logit_scale` (Temperature Scaling): This parameter is used to control the sharpness or concentration of the distribution over classes produced by the model. By applying an exponential function to `logit_scale` and then multiplying it with the cosine similarity scores, we effectively adjust the distribution of these scores. When the scale is high, small differences in cosine similarity lead to larger differences in the resulting values, making the model's predictions more confident (i.e., it amplifies differences between the scores). This is akin to temperature scaling in softmax, where a lower \"temperature\" makes the softmax output more concentrated around the highest value.\n",
    "\n",
    "  - `logit_bias`: The bias term is added to the scaled cosine similarity scores to adjust the baseline level of these scores. It can help in calibrating the model's predictions, especially in scenarios where there might be a systematic bias in the scores that needs correction. Adding a bias term can shift the logits in a way that can improve the model's accuracy or its calibration on specific tasks.\n",
    "\n",
    "These parameters allow for fine-tuning the model's output to better match the target distribution of scores or probabilities needed for specific downstream tasks. Essentially, they provide a means for dynamically adjusting the model's confidence and calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using logit_scale = 4.768828868865967 and logit_bias = -12.933440208435059\n",
      "Cosine similarity per text (2, 1): [[-0.9578456878662109], [-9.76994800567627]]\n",
      "Cosine similarity per image (1, 2): [[-0.9578456878662109, -9.76994800567627]]\n"
     ]
    }
   ],
   "source": [
    "# Normalized features\n",
    "image_embeds = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "text_embeds = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# Cosine similarity as logits\n",
    "print(f\"Using logit_scale = {clip_model.logit_scale.item()} and logit_bias = {clip_model.logit_bias.item()}\")\n",
    "\n",
    "logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * clip_model.logit_scale.exp() + clip_model.logit_bias\n",
    "logits_per_image = logits_per_text.t()\n",
    "\n",
    "print(f\"Cosine similarity per text {tuple(logits_per_text.shape)}: {logits_per_text.tolist()}\")\n",
    "print(f\"Cosine similarity per image {tuple(logits_per_image.shape)}: {logits_per_image.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a model like SigLIP uses the sigmoid function instead of softmax for training, it typically treats the task more as independent binary classifications for each label, rather than as a single multi-class classification problem. \n",
    "\n",
    "To transform cosine similarity scores into probabilities in the context of using a sigmoid function, you can apply the sigmoid function directly to the adjusted cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.73% that image 0 is 'a photo of 2 cats'\n",
      "0.01% that image 0 is 'a photo of 2 dogs'\n"
     ]
    }
   ],
   "source": [
    "probs_per_image = torch.sigmoid(logits_per_image)\n",
    "\n",
    "print(f\"{probs_per_image[0][0]:.2%} that image 0 is '{texts[0]}'\")\n",
    "print(f\"{probs_per_image[0][1]:.2%} that image 0 is '{texts[1]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want to do all this, we can get the similarity (and the embeddings) in a single call using the `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output has: ['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output']\n",
      "27.73% that image 0 is 'a photo of 2 cats'\n",
      "0.01% that image 0 is 'a photo of 2 dogs'\n"
     ]
    }
   ],
   "source": [
    "# CLIP inference with image and text\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n",
    "\n",
    "inputs = clip_processor(text=text, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_model_output = clip_model(**inputs)\n",
    "\n",
    "print(f\"Output has: {list(clip_model_output.keys())}\")\n",
    "\n",
    "# The output contains the embeddings of the image and text and the similarity.\n",
    "assert torch.equal(clip_model_output.text_embeds, text_embeds)\n",
    "assert torch.equal(clip_model_output.image_embeds, image_embeds)\n",
    "assert torch.equal(clip_model_output.logits_per_text, logits_per_text)\n",
    "assert torch.equal(clip_model_output.logits_per_image, logits_per_image)\n",
    "\n",
    "# Calculate the probabilities\n",
    "probs_per_image = torch.sigmoid(clip_model_output.logits_per_image)\n",
    "\n",
    "print(f\"{probs_per_image[0][0]:.2%} that image 0 is '{texts[0]}'\")\n",
    "print(f\"{probs_per_image[0][1]:.2%} that image 0 is '{texts[1]}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the configuration, architecture, the number of parameters of the model and the memory it needs using the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP configuration:\n",
      "SiglipConfig {\n",
      "  \"_name_or_path\": \"./models/siglip-base-patch16-384\",\n",
      "  \"architectures\": [\n",
      "    \"SiglipModel\"\n",
      "  ],\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"model_type\": \"siglip\",\n",
      "  \"text_config\": {\n",
      "    \"model_type\": \"siglip_text_model\"\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"vision_config\": {\n",
      "    \"image_size\": 384,\n",
      "    \"model_type\": \"siglip_vision_model\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Model architecture:\n",
      "SiglipModel(\n",
      "  (text_model): SiglipTextTransformer(\n",
      "    (embeddings): SiglipTextEmbeddings(\n",
      "      (token_embedding): Embedding(32000, 768)\n",
      "      (position_embedding): Embedding(64, 768)\n",
      "    )\n",
      "    (encoder): SiglipEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x SiglipEncoderLayer(\n",
      "          (self_attn): SiglipAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): SiglipMLP(\n",
      "            (activation_fn): PytorchGELUTanh()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (vision_model): SiglipVisionTransformer(\n",
      "    (embeddings): SiglipVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
      "      (position_embedding): Embedding(576, 768)\n",
      "    )\n",
      "    (encoder): SiglipEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x SiglipEncoderLayer(\n",
      "          (self_attn): SiglipAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): SiglipMLP(\n",
      "            (activation_fn): PytorchGELUTanh()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): SiglipMultiheadAttentionPoolingHead(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): SiglipMLP(\n",
      "        (activation_fn): PytorchGELUTanh()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of parameters: 203447810\n",
      "Memory requirement (in Bytes) for full-precision (float32) model: 813796360\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "print(f\"CLIP configuration:\\n{clip_model.config}\")\n",
    "\n",
    "# Model architecture\n",
    "print(f\"Model architecture:\\n{clip_model}\")\n",
    "\n",
    "# Number of parameters\n",
    "print(f\"Number of parameters: {clip_model.num_parameters()}\")\n",
    "\n",
    "# Memory requirements (in Bytes)\n",
    "print(f\"Memory requirement (in Bytes) for full-precision (float32) model: {clip_model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract vision model\n",
    "If we want to use the image part of the model, we can get rid of the text part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model output has: ['last_hidden_state', 'pooler_output']\n",
      "Image features (pooler_output) shape: (1, 768)\n",
      "Image features from CLIP model: tensor([ 0.4710,  0.0673, -0.0466,  0.1934, -0.1196])...\n",
      "Image features from Vision model: tensor([ 0.4710,  0.0673, -0.0466,  0.1934, -0.1196])...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Get the vision model\n",
    "vision_model = clip_model.vision_model\n",
    "\n",
    "# We do inference using the __call__ method\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    vision_model_output = vision_model(**inputs)\n",
    "\n",
    "print(f\"Vision model output has: {list(vision_model_output.keys())}\")\n",
    "\n",
    "# The embedding of the image is stored at pooler_output\n",
    "print(f\"Image features (pooler_output) shape: {tuple(vision_model_output.pooler_output.shape)}\")\n",
    "\n",
    "# It's the same embedding as before\n",
    "assert torch.equal(image_features, vision_model_output.pooler_output)\n",
    "print(f\"Image features from CLIP model: {image_features[0,:5]}...\")\n",
    "print(f\"Image features from Vision model: {vision_model_output.pooler_output[0,:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the configuration and architecture of the model using the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model configuration:\n",
      "SiglipVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"model_type\": \"siglip_vision_model\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"transformers_version\": \"4.38.2\"\n",
      "}\n",
      "\n",
      "Model architecture:\n",
      "SiglipVisionTransformer(\n",
      "  (embeddings): SiglipVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
      "    (position_embedding): Embedding(576, 768)\n",
      "  )\n",
      "  (encoder): SiglipEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x SiglipEncoderLayer(\n",
      "        (self_attn): SiglipAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SiglipMLP(\n",
      "          (activation_fn): PytorchGELUTanh()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): SiglipMultiheadAttentionPoolingHead(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): SiglipMLP(\n",
      "      (activation_fn): PytorchGELUTanh()\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "print(f\"Vision model configuration:\\n{vision_model.config}\")\n",
    "\n",
    "# Model architecture\n",
    "print(f\"Model architecture:\\n{vision_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
