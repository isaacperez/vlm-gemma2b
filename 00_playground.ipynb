{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM (Gemma-2B-it)\n",
    "Documentation:\n",
    "- [Gemma-2B-it](https://huggingface.co/google/gemma-2b-it)\n",
    "- [AutoTokenizer](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer)\n",
    "  - [GemmaTokenizer](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/gemma#transformers.GemmaTokenizer)\n",
    "  - [Tokenizer call](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__)\n",
    "- [AutoModelForCausalLM](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoModelForCausalLM)\n",
    "  - [GemmaModel](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/gemma#transformers.GemmaModel)\n",
    "  - [Model generation](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for LLM\n",
    "llm_model_id = \"google/gemma-2b-it\"\n",
    "llm_model_folder = f\"./models/{llm_model_id.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model 'google/gemma-2b-it' to './models/gemma-2b-it'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d06f6016c4b4546bc0a769106536789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf0e6e349f44c5e8cd715c5e9745ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma-2b-it.gguf:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/isaacperezborrero/Documents/vlm-gemma2b/models/gemma-2b-it'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# First, go to the model's web site and apply to access the model (https://huggingface.co/google/gemma-2b-it). Once you \n",
    "# are granted access, use huggingface-cli for authentication (https://huggingface.co/docs/huggingface_hub/guides/cli)\n",
    "\n",
    "# Speed up file transfers with the Hub\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "\n",
    "# Download the model\n",
    "print(f\"Downloading model '{llm_model_id}' to '{llm_model_folder}'\")\n",
    "snapshot_download(llm_model_id, local_dir=llm_model_folder, local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3b9ae19ed945b8b9ec534e7d1263b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_folder, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Tokenizer\n",
    "The definition of tokenization, as given by Stanford NLP group is:\n",
    "\n",
    "_Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation_\n",
    "\n",
    "Tokenizer used in `Gemma-2B-it` is based on [byte-level Byte-Pair-Encoding](https://github.com/huggingface/transformers/blob/092f1fdaa4224fdd88c616dc9678e6fcb37bfffd/src/transformers/models/gemma/tokenization_gemma.py#L39). Each string is split according this algorithm and each part is assigned to a token.\n",
    "\n",
    "A token is represented as an embedding vector internally in the model. During training, the model learns the values for the embedding.\n",
    "\n",
    "Each token has an identifier that allows to map the string to the corresponding embedding.\n",
    "\n",
    "The vocabulary (number of tokens) of `Gemma-2B-it` is `256000` tokens and each token uses a `2048`-dimensional embedding vector.\n",
    "\n",
    "We can transform strings into tokens (represented as integers) and the other way around. This can be achieved with the\n",
    "methods:\n",
    "  - [`encode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode): `str` -> `list[int, ...]`\n",
    "  - [`decode`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode): `list[int, ...]` -> `str`\n",
    "\n",
    "The model uses some special tokens to indicate that it is the beginning of a sentence or that it ends the user prompt among other uses. These are the special tokens used by `Gemma-2B-it`:\n",
    "  - `<bos>`: Stands for \"beginning of sentence.\" This token is used to indicate the start of a sentence or text sequence. It signals the model to start generating or processing text.\n",
    "  - `<eos>`: Stands for \"end of sentence.\" This token is used to signify the end of a sentence or text sequence. It helps the model determine when to stop generating text or when a complete thought has been processed.\n",
    "  - `<pad>`: Stands for \"padding.\" This token is used to fill in blank spaces in text sequences to ensure they all have the same length when processed in batches. Padding is necessary because many machine learning models require input data of a consistent size.\n",
    "  - `<unk>`: Stands for \"unknown.\" This token is used to represent words or characters that are not found in the model's vocabulary. It acts as a placeholder for any unrecognized or out-of-vocabulary elements.\n",
    "  - `<start_of_turn>`: This token is used to indicate the beginning of a speaker's turn or a new segment of conversation.\n",
    "  - `<end_of_turn>`: Similar to `<start_of_turn>`, this token indicates the end of a speaker's turn or conversation segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of 256000 tokens\n",
      "Special tokens: ['<bos>', '<eos>', '<unk>', '<pad>', '<start_of_turn>', '<end_of_turn>']\n",
      "Mapping of some tokens:\n",
      "Token '<bos>' correspond to id '2'\n",
      "Token '<eos>' correspond to id '1'\n",
      "Token '<unk>' correspond to id '3'\n",
      "Token '<pad>' correspond to id '0'\n",
      "Token '<start_of_turn>' correspond to id '106'\n",
      "Token '<end_of_turn>' correspond to id '107'\n",
      "Token '0' correspond to id '235276'\n",
      "Token '1' correspond to id '235274'\n",
      "Token '2' correspond to id '235284'\n",
      "Token 'a' correspond to id '235250'\n",
      "Token 'b' correspond to id '235268'\n",
      "Token 'c' correspond to id '235260'\n",
      "Token 'the' correspond to id '1175'\n",
      "Token '\\n\\n' correspond to id '109'\n",
      "Encoding:\n",
      "'the0a1b' is encoded as [2, 1175, 235276, 235250, 235274, 235268]\n",
      "[2, 1175, 235276, 235250, 235274, 235268] is decoded as '<bos>the0a1b'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using a vocabulary of {llm_tokenizer.vocab_size} tokens\")\n",
    "\n",
    "# Special tokens (some of them already exists on HuggingFace library and others are new from Gemma)\n",
    "hf_special_tokens = [llm_tokenizer.bos_token, llm_tokenizer.eos_token, llm_tokenizer.unk_token, llm_tokenizer.pad_token]\n",
    "gemma_special_tokens = llm_tokenizer.additional_special_tokens\n",
    "special_tokens = hf_special_tokens + gemma_special_tokens\n",
    "print(f\"Special tokens: {special_tokens}\")\n",
    "\n",
    "# Print some tokens and their ids\n",
    "vocab = llm_tokenizer.get_vocab()\n",
    "print(\"Mapping of some tokens:\")\n",
    "for key in special_tokens + ['0', '1', '2', 'a', 'b', 'c', 'the', '\\n\\n']:\n",
    "    print(f\"Token {repr(key)} correspond to id '{vocab[key]}'\")  # also valid: llm_tokenizer.token_to_id(key)\n",
    "\n",
    "# Let's encode a string\n",
    "text = \"the0a1b\"\n",
    "text_encoded = llm_tokenizer.encode(text)\n",
    "\n",
    "# It adds the <bos> token at the begining. To disable it, pass add_special_tokens=False to encode method.\n",
    "print(\"Encoding:\")\n",
    "print(f\"'{text}' is encoded as {text_encoded}\")\n",
    "\n",
    "# Decode the tokens back to a string\n",
    "text_decoded = llm_tokenizer.decode(text_encoded)\n",
    "print(f\"{text_encoded} is decoded as '{text_decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need the id of the tokens so the model can use its embedding and the __attention mask__. The primary use of the attention mask is to allow the model to differentiate between the actual content and padding within the input sequences. \n",
    "\n",
    "In NLP tasks, input sequences can vary in length. However, most neural networks require inputs to be of a fixed size. To address this, sequences are often padded with special tokens to reach a uniform length before being fed into a model. While necessary for processing, these padding tokens should not influence the model's predictions. The attention mask tells the model which parts of the input are actual data and which are padding. The attention mask is a binary mask (i.e., consisting of zeros and ones) indicating which tokens in the sequence are padding tokens and which are not. For most models, a 1 indicates a real token and a 0 indicates a padding token. During the attention calculation in the model, the mask is used to virtually eliminate the effect of padding tokens by setting their attention scores to a very large negative value (making their resulting softmax scores close to zero). This way, when the softmax function is applied to the attention scores, the padding tokens do not contribute to the final output.\n",
    "\n",
    "Consider a scenario where you have two sentences:\n",
    "\n",
    "- Sentence A: `\"Hello, how are you?\"`\n",
    "- Sentence B: `\"Good morning.\"`\n",
    "\n",
    "If you need to pad these sentences to a fixed length of 5 tokens each, your input might look like this after tokenization and padding (assuming `[PAD]` is the padding token):\n",
    "\n",
    "- Sentence A Tokens: `[Hello, how, are, you, ?]`\n",
    "- Sentence B Tokens: `[Good, morning, [PAD], [PAD], [PAD]]`\n",
    "\n",
    "Correspondingly, the attention mask for these inputs would be:\n",
    "\n",
    "- Sentence A Mask: `[1, 1, 1, 1, 1]` (indicating all tokens should be attended to)\n",
    "- Sentence B Mask: `[1, 1, 0, 0, 0]` (indicating only the first two tokens are real and the rest are padding)\n",
    "\n",
    "By using the attention mask, the model knows to focus on the meaningful content and ignore the padding, thus ensuring more accurate processing and analysis of the input data.\n",
    "\n",
    "We can prepare everything we need to train the model using the Tokenizer call method (`__call__`). The `__call__` method is a more flexible and feature-rich interface to the tokenizer. When you use the tokenizer object like a function (i.e., tokenizer(\"Your input text here.\")), you're actually invoking its `__call__` method. This method can perform tokenization (similar to `encode`), but it also handles additional features like padding the input to a fixed length, truncating inputs to the model's maximum length, returning tensors ready to feed into a model, and more. Essentially, it's designed to prepare the model inputs in one step.\n",
    "\n",
    "The `__call__` method often returns a dictionary containing various keys such as `input_ids`, `token_type_ids`, and `attention_mask`, depending on the configuration and the needs of the specific model you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer output for two sentences: ['the', 'the0a1b']\n",
      "input_ids (2, 6): tensor([[     0,      0,      0,      0,      2,   1175],\n",
      "        [     2,   1175, 235276, 235250, 235274, 235268]])\n",
      "attention_mask (2, 6): tensor([[0, 0, 0, 0, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Since we plan to use the tokenizer to train the model and the model only accepts Pytorch Tensors as input, we need to\n",
    "# set return_tensors=\"pt\". We also need to add padding because the sentences are not the same length.\n",
    "sentences = [\"the\", \"the0a1b\"]\n",
    "tokenizer_output = llm_tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(f\"Tokenizer output for two sentences: {sentences}\")\n",
    "for k, v in tokenizer_output.items():\n",
    "    print(f\"{k} {tuple(v.shape)}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemma-2B-it is an instruction model. When a language model (LLM) is described as an \"instruction model,\" it refers to a type of language model that is specifically trained or fine-tuned to follow or understand natural language instructions and generate responses or outputs based on those instructions. \n",
    "\n",
    "Instruction models are typically created by training or fine-tuning large language models on datasets that contain pairs of instructions and corresponding outputs or actions. \n",
    "\n",
    "Beyond serving as instruction models, LLMs are increasingly employed for facilitating chat-based interactions. In such scenarios, rather than generating continuations for a singular text string, these models engage in conversations. These interactions are structured as sequences of messages, each tagged with a role—such as \"user\" or \"assistant\"—and accompanied by the corresponding message text. This approach enables the model to understand and maintain the flow of dialogue, distinguishing between different speakers and their messages.\n",
    "\n",
    "Much like tokenization, different models expect very different input formats for chat. This is the reason HuggingFace added __chat templates__ as a feature. Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects.\n",
    "\n",
    "Chat template are written using [Jinja templates](https://jinja.palletsprojects.com/en/3.1.x/templates/).\n",
    "\n",
    "__Gemma-2B-it is not a chat model__ but Huggingface sets a default chat template to each model so that we can inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}\n",
      "{% if messages[0]['role'] == 'system' %}\n",
      "{{ raise_exception('System role not supported') }}\n",
      "{% endif %}\n",
      "{% for message in messages %}\n",
      "{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
      "{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "{% endif %}\n",
      "{% if (message['role'] == 'assistant') %}\n",
      "{% set role = 'model' %}\n",
      "{% else %}\n",
      "{% set role = message['role'] %}\n",
      "{% endif %}\n",
      "{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}\n",
      "{% endfor %}\n",
      "{% if add_generation_prompt %}\n",
      "{{'<start_of_turn>model\n",
      "'}}\n",
      "{% endif %}\n",
      "Chat template applied:\n",
      "<bos><start_of_turn>user\n",
      "Hello, how are you?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "I'm doing great. How can I help you today?<end_of_turn>\n",
      "<start_of_turn>user\n",
      "I'd like to show off how chat templating works!<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The chat template for a model is stored on the tokenizer.chat_template attribute\n",
    "chat_template = llm_tokenizer.chat_template\n",
    "print(\"}\\n{\".join(chat_template.split(\"}{\")))\n",
    "\n",
    "# We can apply the template to any chat. A chat consists of a list of JSONs with the role and the message content\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "\n",
    "]\n",
    "\n",
    "print(f\"Chat template applied:\\n{llm_tokenizer.apply_chat_template(chat, tokenize=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have not used embeddings. We have converted the tokens (text) to their identifiers with the `encode` method and then converted them back to text with the `decode` method. When you use __call__ method does the same as `encode`, it just has some extra functionality. \n",
    "\n",
    "To obtain the embedding of a token you typically don't use the Tokenizer class directly for embeddings. Instead, the Tokenizer class is used to convert text to tokens or token IDs, which are then fed into a model to obtain embeddings. __The embeddings themselves are retrieved from the model, not the tokenizer__.\n",
    "\n",
    "In the case of `Gemma-2B-it`, the embedding model is stored at `model.embed_tokens`. It's a Pytorch `nn.Embedding` layer, which is a simple lookup table that stores embeddings of a fixed dictionary and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'the' token (id 1175) with shape torch.Size([1, 2048]): tensor([[ 0.1924, -0.0713, -0.0732,  ...,  0.0156, -0.0017,  0.0288]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Embedding for 'the' token\n",
    "embedding = llm_model.model.embed_tokens(torch.LongTensor([1175]))\n",
    "print(f\"Embedding for 'the' token (id 1175) with shape {embedding.shape}: {embedding.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add new tokens to the tokenizer without training it again.\n",
    "\n",
    "The new token we want to add can be a special token. They play a critical role in signaling specific types of information or instructions to the model. They help the model understand the structure of the input data, differentiate between various segments of data, and perform specific actions based on the type of token encountered.\n",
    "\n",
    "For example, Padding tokens (`[PAD]`) are used to ensure that all input sequences in a batch have the same length. They are ignored by the model during processing, allowing sequences of varying lengths to be batched together without affecting the model's performance.\n",
    "\n",
    "When we add a new token we have to modify the Tokenizer and the embedding layer of the LLM. First we have to add the new token to the Tokenizer so that it can be identified, and then we have to modify the embedding layer to add a new random embedding for this new token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id for 'new_token' token before: [1404, 235298, 5526]\n",
      "token id for 'new_token' token after: [256000]\n",
      "Embedding for 'the' and 'new_token' tokens (id 1175 and 256000) with shape torch.Size([2, 2048]):tensor([[ 0.1924, -0.0713, -0.0732,  ...,  0.0156, -0.0017,  0.0288],\n",
      "        [ 0.0056, -0.0140, -0.0007,  ..., -0.0064, -0.0337, -0.0594]])\n",
      "token id for '<image>' token: [2, 256001]\n",
      "Tokenizer output for sentence: <image> What do you see in this picture?\n",
      "input_ids (1, 10): tensor([[     2, 256001,   2439,    749,    692,   1443,    575,    736,   5642,\n",
      "         235336]])\n",
      "attention_mask (1, 10): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# new tokens \n",
    "new_tokens = [\"new_token\"]\n",
    "\n",
    "# check if the tokens are already in the vocabulary\n",
    "assert len(set(new_tokens) - set(llm_tokenizer.vocab.keys())) > 0\n",
    "\n",
    "# add the tokens to the tokenizer vocabulary\n",
    "print(f\"token id for '{new_tokens[0]}' token before: {llm_tokenizer.encode(new_tokens[0], add_special_tokens=False)}\")\n",
    "llm_tokenizer.add_tokens(list(new_tokens))\n",
    "print(f\"token id for '{new_tokens[0]}' token after: {llm_tokenizer.encode(new_tokens[0], add_special_tokens=False)}\")\n",
    "\n",
    "# add new, random embeddings for the new tokens\n",
    "llm_model.resize_token_embeddings(len(llm_tokenizer))\n",
    "\n",
    "# The old embeddings are the same, but we have a new one for the new token\n",
    "embedding = llm_model.model.embed_tokens(torch.LongTensor([1175, 256000]))\n",
    "print(f\"Embedding for 'the' and 'new_token' tokens (id 1175 and 256000) with shape {embedding.shape}:\" \n",
    "      f\"{embedding.detach()}\")\n",
    "\n",
    "# If we want to add a new special token, we just have to set special_token=True in the Tokenizer\n",
    "new_special_tokens = [\"<image>\"]\n",
    "llm_tokenizer.add_tokens(new_special_tokens, special_tokens=True)\n",
    "llm_model.resize_token_embeddings(len(llm_tokenizer))\n",
    "print(f\"token id for '{new_special_tokens[0]}' token: {llm_tokenizer.encode(new_special_tokens[0])}\")  # 2: <bos>\n",
    "\n",
    "sentence = \"<image> What do you see in this picture?\"\n",
    "tokenizer_output = llm_tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(f\"Tokenizer output for sentence: {sentence}\")\n",
    "for k, v in tokenizer_output.items():\n",
    "    print(f\"{k} {tuple(v.shape)}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do inference with the LLM we only need to pass the output of the tokenizer to the `generate` method of the model (`input_ids` and, optionally, `attention_mask`).\n",
    "The output of the LLM is a dictionary with the input_ids of the tokens, so we need to use the tokenizer to decode it and create the final text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model's output (id of the tokens): tensor([[     2,   1841,    603,    573,  12324,   8180,    575,    573,   2134,\n",
      "         235336,    109,  24059,  99771, 235269,   7023,    575,    573, 148783,\n",
      "         235269,    603,    573,   9393,   8180,    611,  10379, 235269,    675,\n",
      "            476,  14538,  28554,    576, 235248, 235321, 235269, 235321, 235310,\n",
      "         235321, 235265, 235321, 235318,  18678,    591, 235284, 235315, 235269,\n",
      "         235276, 235304, 235284, 235265, 235310,   5368,    846,      1]])\n",
      "Final output with special tokens: <bos>What is the biggest mountain in the world?\n",
      "\n",
      "Mount Everest, located in the Himalayas, is the highest mountain on Earth, with a peak elevation of 8,848.86 meters (29,032.4 feet).<eos>\n",
      "Final output without special tokens: What is the biggest mountain in the world?\n",
      "\n",
      "Mount Everest, located in the Himalayas, is the highest mountain on Earth, with a peak elevation of 8,848.86 meters (29,032.4 feet).\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the biggest mountain in the world?\"\n",
    "inputs = llm_tokenizer(text, return_tensors=\"pt\")\n",
    "generate_ids = llm_model.generate(**inputs, max_length=256)\n",
    "\n",
    "# The output of the model is in batch format so we use batch_decode instead of decode to get the text. Also, we set \n",
    "# skip_special_tokens=True to remove all special tokens from the output (like <bos>). The output is a list with the \n",
    "# decoded text, so we only need the first element of the list ([0]), as we only have one sentence.\n",
    "text_output_with_special_tokens = llm_tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "text_output_without_special_tokens = llm_tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(f\"Raw model's output (id of the tokens): {generate_ids}\")\n",
    "print(f\"Final output with special tokens: {text_output_with_special_tokens}\")\n",
    "print(f\"Final output without special tokens: {text_output_without_special_tokens}\")\n",
    "\n",
    "# Note that the output also contains the input text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the configuration, architecture, the number of parameters of the model and the memory it needs using the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM configuration:\n",
      "GemmaConfig {\n",
      "  \"_name_or_path\": \"./models/gemma-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "Model architecture:\n",
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n",
      "Number of parameters: 2506172416\n",
      "Memory requirement (in Bytes): 10091807744\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "print(f\"LLM configuration:\\n{llm_model.config }\")\n",
    "\n",
    "# Model architecture\n",
    "print(f\"Model architecture:\\n{llm_model}\")\n",
    "\n",
    "# Number of parameters\n",
    "print(f\"Number of parameters: {llm_model.num_parameters()}\")\n",
    "\n",
    "# Memory requirements (in Bytes)\n",
    "print(f\"Memory requirement (in Bytes): {llm_model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual encoder (CLIP model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
