{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a VLM\n",
    "To develop a __Visual Language Model__ (VLM), it's essential to handle both textual and visual inputs. For text, we utilize a Large Language Model (LLM), while images are processed through the CLIP model. It's important to note, though, that CLIP includes a minor language component that is superfluous for our needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for LLM\n",
    "llm_model_id = \"google/gemma-2b-it\"\n",
    "llm_model_folder = f\"./models/{llm_model_id.split('/')[-1]}\"\n",
    "\n",
    "# Global constants for CLIP model\n",
    "clip_model_id = \"google/siglip-base-patch16-384\"\n",
    "clip_model_folder = f\"./models/{clip_model_id.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc537eb4c2f24dfebb3ab8a54ceb3d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(clip_model_folder, local_files_only=True)\n",
    "clip_processor = AutoProcessor.from_pretrained(clip_model_folder, local_files_only=True)\n",
    "\n",
    "# Get the vision model and the image processor from CLIP\n",
    "clip_vision_model = clip_model.vision_model\n",
    "image_processor = clip_processor.image_processor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the components\n",
    "\n",
    "CLIP typically generates a singular embedding to represent an entire image. This approach is less desirable for our purposes, as we favor maintaining a sequence of image embeddings. This sequence-based approach preserves more detailed information within the image, as opposed to condensing it into a single, less descriptive vector.\n",
    "\n",
    "In the CLIP model, an image goes through this whole process:\n",
    "\n",
    "1. __Preprocessing of the Image__:\n",
    "  - The input image is first resized to the specified `image_size` of 384x384 pixels.\n",
    "    It’s then divided into patches of `patch_size` 16x16 pixels. This results in a grid of patches (24x24 = 576 patches for a 384x384 image), as the stride matches the patch size, ensuring no overlap.\n",
    "\n",
    "2. __Patch Embedding__:\n",
    "  - Each patch is flattened and passed through a convolutional layer (`Conv2d`) that acts as a patch embedding layer. This converts each patch into a 768-dimensional vector (`hidden_size` = 768). This step transforms the spatial patch information into a format suitable for processing by the subsequent transformer layers.\n",
    "\n",
    "3. __Position Embedding__:\n",
    "  - Positional embeddings are added to the patch embeddings to retain information about the original position of each patch within the image. The `position_embedding` component embeds the sequential position of each patch into its representation.\n",
    "\n",
    "4. __SiglipEncoder Processing__:\n",
    "  - The resulting embedded patches, now augmented with positional information, are passed through the `SiglipEncoder`. This encoder consists of multiple (`num_hidden_layers` = 12) identical layers, each comprising:\n",
    "    - __Self-Attention Mechanism__: Each layer contains a `SiglipAttention` module that computes self-attention for each patch embedding, allowing the model to weigh the importance of different patches based on their content and relation to other patches.\n",
    "    - __Intermediate Feed-Forward Network (MLP)__: After attention computation, the data passes through a two-layer feed-forward network (MLP) with a GELU-Tanh activation function (`hidden_act = \"gelu_pytorch_tanh\"`) between the layers, which allows for non-linear processing of each patch's information.\n",
    "    - __Layer Normalization__: Each attention and MLP operation is followed by layer normalization (`layer_norm_eps = 1e-06`) to stabilize the learning process and improve convergence.\n",
    "\n",
    "5. __Post Layer Normalization__:\n",
    "  - After processing through the encoder, a final layer normalization step is applied to the output of the last encoder layer to ensure that the output features are normalized before passing to the classification head.\n",
    "\n",
    "6. __Classification Head__:\n",
    "  - The `SiglipMultiheadAttentionPoolingHead` combines the features from all patches to form a single coherent representation of the image. This involves another round of attention to pool information across patches, followed by layer normalization and a final MLP for processing.\n",
    "\n",
    "\n",
    "By the time the input data has passed through all the encoder layers, it has been transformed into a sequence of embeddings. Each embedding in this sequence corresponds to a patch of the original image, but now represents a deeply processed, high-dimensional feature vector encapsulating both the intrinsic properties of the patch and its contextual relationships with other patches in the image. \n",
    "\n",
    "At stage 5, we're left with a series of embedding vectors, each 768 dimensions in size, that collectively depict the processed image. CLIP, however, simplifies this array into a single embedding vector for subsequent comparison with text embeddings. This simplification occurs in stage 6, where the model aggregates the detailed sequence of image embeddings into one cohesive vector that captures the core attributes of the image. An attention layer along with a Multilayer Perceptron (MLP) is employed to forge this final image representation. From the resultant sequence, only the initial vector is retained, serving as the all-encompassing embedding for the image. \n",
    "\n",
    "To keep the whole sequence of embedding, HuggingFace allows us to access the intermediate outputs of the model so that we can retrieve the output we want. You only have to pass `output_hidden_states=True` when you call the model. It will return the hidden states of the model at the output of each layer plus the initial embedding outputs.\n",
    "\n",
    "The problem is that you don't know where they come from. To know which layer corresponds to which output you have to look at the model implementation in HuggingFace and trace `output_hidden_states` through all the submodules to find out which ones are used. For SigLIP the code is [here](https://github.com/huggingface/transformers/blob/df1542581ee89107eea0569ee044fa8797b66ab0/src/transformers/models/siglip/modeling_siglip.py). It uses 13 layers: input embeddings + 12 layers from `SiglipEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model output has: ['last_hidden_state', 'pooler_output', 'hidden_states']\n",
      "We have the output of 13 layers\n",
      "Output shape of layer 1: (1, 576, 768)\n",
      "Output shape of layer 2: (1, 576, 768)\n",
      "Output shape of layer 3: (1, 576, 768)\n",
      "Output shape of layer 4: (1, 576, 768)\n",
      "Output shape of layer 5: (1, 576, 768)\n",
      "Output shape of layer 6: (1, 576, 768)\n",
      "Output shape of layer 7: (1, 576, 768)\n",
      "Output shape of layer 8: (1, 576, 768)\n",
      "Output shape of layer 9: (1, 576, 768)\n",
      "Output shape of layer 10: (1, 576, 768)\n",
      "Output shape of layer 11: (1, 576, 768)\n",
      "Output shape of layer 12: (1, 576, 768)\n",
      "Output shape of layer 13: (1, 576, 768)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Return the intermediate outputs with output_hidden_states=True\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    clip_vision_model_output = clip_vision_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "print(f\"Vision model output has: {list(clip_vision_model_output.keys())}\")\n",
    "\n",
    "# The intermediate outputs are stored at hidden_states\n",
    "hidden_states = clip_vision_model_output.hidden_states\n",
    "print(f\"We have the output of {len(hidden_states)} layers\")\n",
    "\n",
    "# We have one output for the input embedding and one for each layer of the SiglipEncoder. The output shape is always\n",
    "# (batch size, num. patches, embedding size). We use patches of 16x16 pixels for an image of 384x384 => 576 patches\n",
    "for idx, hidden_output in enumerate(hidden_states):\n",
    "    print(f\"Output shape of layer {idx + 1}: {tuple(hidden_output.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with using `output_hidden_states=True` is that it requires too much memory to store all the hidden states. If we only want to get one of them, we can create a new model that only stores the output we need. Also, all the computation that is done after that layer is unnecessary, so we can remove these modules to reduce the memory used by the model and the computations it has to perform.\n",
    "\n",
    "We can inspect the implementation of the vision encoder of SigLIP (`SiglipVisionTransformer`):\n",
    "```python\n",
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = SiglipVisionEmbeddings(config)\n",
    "        self.encoder = SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "        self.head = SiglipMultiheadAttentionPoolingHead(config)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(SIGLIP_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=SiglipVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "\n",
    "        pooled_output = self.head(last_hidden_state)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "```\n",
    "\n",
    "As we can see it is just a Pytorch Module. It consists of four components:\n",
    "- `embedding`: This is the part of the model responsible for converting raw pixel values of the input images into a higher-dimensional vector space.\n",
    "- `encoder`: The encoder is the core of the model. It processes the sequence of embedded patches through several layers of self-attention and feed-forward neural networks.\n",
    "- `post_layernorm`: it is used to apply normalization after the encoder to ensure that the output across different patches is normalized before any further processing.\n",
    "- `head`: the \"head\" refers to the component that takes the output of the transformer encoder and performs a specific task, such as classification. In the context of `SiglipVisionTransformer`, this head is designed for pooling the transformer outputs into a single vector representation of the input image.\n",
    "\n",
    "If we want the last layer of the encoder, we can get rid of the `post_layernorm` and the `head` and create a model that does the forward pass with the `embedding` and the `encoder` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model output shape: (1, 576, 768)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class ModifiedSiglipVisionModel(nn.Module):\n",
    "    def __init__(self, ref_clip_vision_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.config = ref_clip_vision_model.config\n",
    "\n",
    "        self.embeddings = ref_clip_vision_model.embeddings\n",
    "        self.encoder = ref_clip_vision_model.encoder\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "\n",
    "        # Convert pixel values to embeddings\n",
    "        embeddings = self.embeddings(pixel_values)\n",
    "\n",
    "        # Process embeddings through the encoder\n",
    "        encoder_outputs = self.encoder(inputs_embeds=embeddings)\n",
    "\n",
    "        # There is only one output: last_hidden_state\n",
    "        sequence_output = encoder_outputs[0]  # `encoder_outputs.last_hidden_state` is the same\n",
    "\n",
    "        return sequence_output\n",
    "\n",
    "\n",
    "# Create the vision model\n",
    "vision_model = ModifiedSiglipVisionModel(ref_clip_vision_model=clip_vision_model)\n",
    "\n",
    "# Do inference with the new model\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    vision_model_output = vision_model(**inputs)\n",
    "\n",
    "# It's the same output as before\n",
    "assert torch.equal(vision_model_output, hidden_states[-1])\n",
    "print(f\"Vision model output shape: {tuple(vision_model_output.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the configuration and architecture of the model. __Note that the configuration does not correspond to this new module but to the original one__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model configuration:\n",
      "SiglipVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"model_type\": \"siglip_vision_model\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"transformers_version\": \"4.38.2\"\n",
      "}\n",
      "\n",
      "Model architecture:\n",
      "ModifiedSiglipVisionModel(\n",
      "  (embeddings): SiglipVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
      "    (position_embedding): Embedding(576, 768)\n",
      "  )\n",
      "  (encoder): SiglipEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x SiglipEncoderLayer(\n",
      "        (self_attn): SiglipAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SiglipMLP(\n",
      "          (activation_fn): PytorchGELUTanh()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "print(f\"Vision model configuration:\\n{vision_model.config}\")\n",
    "\n",
    "# Model architecture\n",
    "print(f\"Model architecture:\\n{vision_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the number of parameters before and after the prunning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before prunning: 93176064\n",
      "Parameters after prunning: 86087424\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parameters before prunning: {sum(p.numel() for p in clip_vision_model.parameters())}\")\n",
    "print(f\"Parameters after prunning: {sum(p.numel() for p in vision_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual and Text connection\n",
    "Now we have a visual model that can produce a sequence of tokens. This sequence can be consumed by the LLM and we can start asking question about\n",
    "the image. Internally, the LLM will use the attention mechanism to look at the part of the image with the relevant information to answer the question.\n",
    "\n",
    "However, the embedding dimension of our visual model (768 dimensions) is not the same as the input embeddinds of the LLM (2048). Therefore, we need to transform the visual model's output to a 2048-dimensional embedding using a MLP or a linear layer. We call this last module, the visual adapter.\n",
    "\n",
    "\n",
    "At this stage, our visual model effectively translates images into a sequence of tokens, creating a bridge for our Language Model (LLM) to interpret and engage with visual data. By enabling the LLM to process these tokens, we can pose questions about the image's content and receive informed responses. This process leverages the LLM's attention mechanism, which selectively focuses on segments of the image encoded in the tokens that are most pertinent to the question at hand.\n",
    "\n",
    "However, a technical challenge arises due to a discrepancy in the dimensionality of the embeddings between our visual model and the LLM. Specifically, the visual model outputs embeddings with a dimensionality of 768, whereas the LLM expects inputs with a dimensionality of 2048 for its embeddings. To bridge this gap, we employ a Multi-Layer Perceptron (MLP) or a linear transformation layer, aptly named the __visual adapter__.\n",
    "\n",
    "The visual adapter serves a critical function: it transforms the 768-dimensional output from the visual model into a format compatible with the LLM's 2048-dimensional input embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape of 'the' token: (1, 2048)\n",
      "Vision model output shape: (1, 576, 768)\n"
     ]
    }
   ],
   "source": [
    "# LLM input size\n",
    "embedding = llm_model.model.embed_tokens(torch.LongTensor([1175]))\n",
    "print(f\"Embedding shape of 'the' token: {tuple(embedding.shape)}\")\n",
    "\n",
    "# Visual encoder output size\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    vision_model_output = vision_model(**inputs)\n",
    "\n",
    "print(f\"Vision model output shape: {tuple(vision_model_output.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to add the visual adapter to the visual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model with adapter output shape: (1, 576, 2048)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ModifiedSiglipVisionModelWithAdapter(nn.Module):\n",
    "    def __init__(self, ref_clip_vision_model: nn.Module, llm_model:transformers.AutoModel):\n",
    "        super().__init__()\n",
    "        self.config = ref_clip_vision_model.config\n",
    "\n",
    "        self.embeddings = ref_clip_vision_model.embeddings\n",
    "        self.encoder = ref_clip_vision_model.encoder\n",
    "\n",
    "        self.adapter = nn.Linear(in_features=self.config.hidden_size, out_features=llm_model.config.hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "\n",
    "        # Convert pixel values to embeddings\n",
    "        embeddings = self.embeddings(pixel_values)\n",
    "\n",
    "        # Process embeddings through the encoder\n",
    "        encoder_outputs = self.encoder(inputs_embeds=embeddings)\n",
    "\n",
    "        # There is only one output: last_hidden_state\n",
    "        sequence_output = encoder_outputs[0]  # `encoder_outputs.last_hidden_state` is the same\n",
    "\n",
    "        # Prepare the output for the LLM\n",
    "        sequence_output = self.adapter(sequence_output)\n",
    "\n",
    "        return sequence_output\n",
    "\n",
    "\n",
    "# Create the vision model\n",
    "vision_model_with_adapter = ModifiedSiglipVisionModelWithAdapter(\n",
    "    ref_clip_vision_model=clip_vision_model, llm_model=llm_model\n",
    ")\n",
    "\n",
    "# Do inference with the new model\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    vision_model_with_adapter_output = vision_model_with_adapter(**inputs)\n",
    "\n",
    "print(f\"Vision model with adapter output shape: {tuple(vision_model_with_adapter_output.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon adapting the visual model's output to match the embedding size required by the LLM, one might assume we can straightforwardly feed these transformed tokens into the LLM and proceed with generating responses. However, a crucial nuance arises at this juncture: the tokens generated from the image, despite being in the correct dimensionality, represent a set of embeddings unfamiliar to the LLM. These embeddings do not correspond to any pre-existing token IDs within the LLM's vocabulary. Essentially, these new embeddings are akin to a foreign language to the LLM, containing potentially any value within the embedding space, without any predefined semantic association that the LLM can recognize.\n",
    "\n",
    "This disparity prevents us from utilizing the standard approach of passing token IDs to the LLM for text generation. Token IDs serve as predefined pointers to specific embeddings within the LLM's vocabulary, acting as the basis for generating meaningful text responses. In the absence of corresponding token IDs for the new image-derived embeddings, this method is not viable.\n",
    "\n",
    "However, there is a solution: bypassing the token ID-based input mechanism in favor of directly injecting the embeddings into the LLM. By sending these adapted embeddings straight to the LLM, we effectively communicate the visual information in a format the LLM can process, despite the embeddings not being part of its initial vocabulary. This approach enables the LLM to leverage its attention and contextual understanding capabilities on the embeddings derived from visual data, allowing it to generate relevant text responses based on the content of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: (1, 576, 2048)\n",
      "Text embedding shape: (1, 4, 2048)\n",
      "Image and text embedding shape: (1, 580, 2048)\n",
      "Raw model's output (id of the tokens): tensor([[   109,    109,    109,    109,    109,    109,    109,    109,    109,\n",
      "            109, 235280,    578, 235279, 235269, 235248, 235248, 235248,    109,\n",
      "            109,    109,    109,    109,    109,    109,    109,    109,    109,\n",
      "            109, 235269, 235248, 235248, 235248, 235248,    109,    109,    109,\n",
      "            109,    109,    109,    109,    109,    109,    109,    108,    688,\n",
      "              1]])\n",
      "Final output without special tokens: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA andT,   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n,    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n**'\n"
     ]
    }
   ],
   "source": [
    "# Do inference with the new model\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    vision_model_with_adapter_output = vision_model_with_adapter(**inputs)\n",
    "\n",
    "# Get the embedding for the text\n",
    "text_embedding = llm_model.model.embed_tokens(\n",
    "    torch.LongTensor(llm_tokenizer.encode(\"Describe the image.\", add_special_tokens=False))\n",
    ")\n",
    "\n",
    "# Add the batch dimension\n",
    "text_embedding = text_embedding.unsqueeze(0)\n",
    "\n",
    "# Combine thte image and the text\n",
    "embeddings = torch.concat([vision_model_with_adapter_output, text_embedding], dim=1)\n",
    "print(f\"Image embedding shape: {tuple(vision_model_with_adapter_output.shape)}\")\n",
    "print(f\"Text embedding shape: {tuple(text_embedding.shape)}\")\n",
    "print(f\"Image and text embedding shape: {tuple(embeddings.shape)}\")\n",
    "\n",
    "# Generate the output of the LLM\n",
    "generate_ids = llm_model.generate(inputs_embeds=embeddings, max_new_tokens=256)\n",
    "text_output_without_special_tokens = llm_tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(f\"Raw model's output (id of the tokens): {generate_ids}\")\n",
    "print(f\"Final output without special tokens: {repr(text_output_without_special_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the output makes no sense because the model has not been trained with these image tokens nor has the visual adapter been trained.\n",
    "\n",
    "To ensure our model can effectively learn from and respond to both visual and textual inputs, we need to develop a function that seamlessly merges image and text embeddings. This is crucial for maintaining consistency across training and inference phases, thereby preventing discrepancies that could negatively impact model performance.\n",
    "\n",
    "At the heart of our approach is the introduction of a special token, designated as a placeholder, which signals the position within the text sequence where image embeddings should be integrated. This system allows us to dynamically insert visual information into the model's input stream, ensuring that both text and image data are processed in a unified manner.\n",
    "\n",
    "Consider a scenario where we wish to query the model about the contents of an image. The input might be structured as follows: `\"<bos><image> What is the content of the image?\"`. Through our custom method, this input is transformed into a concatenated sequence of embeddings: `[\"EMBEDDING_FOR_TOKEN_<bos>\", \"EMBEDDING_FOR_IMAGE_PATCH_1\", ..., \"EMBEDDING_FOR_IMAGE_PATCH_N\", \"EMBEDDING_FOR_TOKEN_What\", ..., \"EMBEDDING_FOR_TOKEN_?\"]`. Each `\"EMBEDDING_FOR_IMAGE_PATCH_X\"` represents an embedding for a segment of the image, allowing the model to consider discrete portions of the visual input alongside textual information.\n",
    "\n",
    "**Detailed Process:**\n",
    "\n",
    "1. **Token Identification**: The method first identifies the special placeholder token within the input sequence. This token acts as a marker for where the image embeddings will be inserted, replacing the placeholder.\n",
    "\n",
    "2. **Embedding Concatenation**: The model then constructs a new input sequence by concatenating the appropriate embeddings. Text embeddings are taken directly from the pre-trained language model's vocabulary, while image embeddings are generated by processing the image through our visual encoder.\n",
    "\n",
    "3. **Sequence Expansion**: Given that an image is represented by multiple embeddings (one for each patch or segment of the image), the input sequence is dynamically expanded to accommodate these additional embeddings. This ensures that the model receives a comprehensive representation of both the textual query and the visual content.\n",
    "\n",
    "This process is applied identically during both training and inference, ensuring that the model consistently interprets and processes the combined input. Such consistency is vital for the model to accurately learn from its training data and to apply this knowledge effectively during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 1 tokens\n",
      "LLM model config:\n",
      "GemmaConfig {\n",
      "  \"_name_or_path\": \"./models/gemma-2b-it\",\n",
      "  \"architectures\": [\n",
      "    \"GemmaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 16384,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 18,\n",
      "  \"num_key_value_heads\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, you'll need to add the special token to your tokenizer.\n",
    "image_placeholder_token = '<image>'\n",
    "special_tokens_dict = {'additional_special_tokens': [image_placeholder_token]}\n",
    "num_added_toks = llm_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(f\"We have added {num_added_toks} tokens\")\n",
    "\n",
    "# After adding the special token to the tokenizer, you need to ensure that the model’s embedding layer is aware of this \n",
    "# new token. This involves resizing the embedding layer to accommodate the additional token(s).\n",
    "llm_model.resize_token_embeddings(len(llm_tokenizer))\n",
    "\n",
    "# Adjust configuration of the model\n",
    "llm_model.config.vocab_size = len(llm_tokenizer)\n",
    "\n",
    "print(f\"LLM model config:\\n{llm_model.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt: '<image>\\nDescribe the image.'\n",
      "Tokenizer input_ids: tensor([[     2, 256000,    108,  50721,    573,   2416, 235265]])\n",
      "Image features shape: (1, 576, 2048)\n",
      "The prompt '<image>\\nDescribe the image.' will have 6 text tokens and 576 image tokens = 582 tokens/embeddings\n",
      "After merging the image and text embeddings the final shape of the embeddings and attention mask is: (1, 582, 2048) - (1, 582)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Now we can create the function that will merge the text and image embeddings\n",
    "def merge_input_ids_with_image_features(image_features, input_ids, attention_mask, llm_model, image_token_id):\n",
    "    # Initialize variables for the new sequence dimensions\n",
    "    batch_size, seq_length = input_ids.shape\n",
    "    image_feature_dim = image_features.size(-1)\n",
    "    image_seq_length = image_features.size(1)\n",
    "\n",
    "    # Convert input_ids to embeddings\n",
    "    inputs_embeds = llm_model.model.embed_tokens(input_ids)\n",
    "\n",
    "    # Device adjustment for new tensors based on the device of inputs_embeds\n",
    "    device = inputs_embeds.device\n",
    "    \n",
    "    # Find positions of the <image> tokens\n",
    "    image_token_mask = input_ids == image_token_id\n",
    "    num_image_tokens_per_seq = torch.sum(image_token_mask, dim=-1)\n",
    "    image_token_positions = image_token_mask.nonzero()\n",
    "\n",
    "    # There must be always one and only one image per sequence\n",
    "    if not all(num_image_tokens_per_seq == 1):\n",
    "        raise RuntimeError(\n",
    "            f\"Expecting one and only one image (token_id = {image_token_id}) per sequence in the batch: {input_ids}\"\n",
    "        )\n",
    "    \n",
    "    # All image tokens must be at the same position\n",
    "    if not all(image_token_positions[0, 1] == image_token_positions[:, 1]):\n",
    "        raise RuntimeError(\n",
    "            f\"Image token (token_id = {image_token_id}) is not at the same position in all sequences of the batch:\"\n",
    "            f\"{input_ids}\"\n",
    "        )\n",
    "\n",
    "    # Calculate the new sequence length after inserting image features\n",
    "    new_seq_length =  image_seq_length + seq_length - 1  # - 1 because we remove the image token\n",
    "\n",
    "    # Prepare containers for the new embeddings and attention mask\n",
    "    new_inputs_embeds = torch.zeros(batch_size, new_seq_length, image_feature_dim, device=device)\n",
    "    new_attention_mask = torch.zeros(batch_size, new_seq_length, device=device)\n",
    "    \n",
    "    # Copy the text and image embeddings into the new containers\n",
    "    for batch_idx, pos in image_token_positions:\n",
    "        # Copy text embeddings up to the image token\n",
    "        if pos > 0:\n",
    "            new_inputs_embeds[:, :pos, :] = inputs_embeds[:, :pos, :]\n",
    "            new_attention_mask[:, :pos] = attention_mask[:, :pos]\n",
    "\n",
    "        # Insert image features\n",
    "        image_start_idx = pos\n",
    "        image_end_idx = image_start_idx + image_seq_length\n",
    "        new_inputs_embeds[batch_idx, image_start_idx:image_end_idx, :] = image_features[batch_idx, :, :]\n",
    "        new_attention_mask[batch_idx, image_start_idx:image_end_idx] = 1\n",
    "\n",
    "        # Copy remaining text embeddings after the <image> token\n",
    "        if pos < seq_length - 1:\n",
    "            remaining_text_start_idx = image_end_idx\n",
    "            new_inputs_embeds[batch_idx, remaining_text_start_idx:, :] = inputs_embeds[batch_idx, pos+1:, :]\n",
    "            new_attention_mask[batch_idx, remaining_text_start_idx:] = attention_mask[batch_idx, pos+1:]\n",
    "\n",
    "    return new_inputs_embeds, new_attention_mask\n",
    "\n",
    "\n",
    "# Get the inputs\n",
    "prompt = f\"{image_placeholder_token}\\nDescribe the image.\"\n",
    "image_token_id = llm_tokenizer.convert_tokens_to_ids(image_placeholder_token)\n",
    "tokenizer_output = llm_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "print(f\"Using prompt: {repr(prompt)}\")\n",
    "print(f\"Tokenizer input_ids: {tokenizer_output.input_ids}\")\n",
    "\n",
    "image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = vision_model_with_adapter(**inputs)\n",
    "\n",
    "print(f\"Image features shape: {tuple(image_features.shape)}\")\n",
    "\n",
    "new_inputs_embeds, new_attention_mask = merge_input_ids_with_image_features(\n",
    "    image_features, tokenizer_output.input_ids, tokenizer_output.attention_mask, llm_model, image_token_id\n",
    ")\n",
    "\n",
    "num_text_tokens = tokenizer_output.input_ids.shape[1] - 1\n",
    "print(f\"The prompt {repr(prompt)} will have {num_text_tokens} text tokens \"\n",
    "      f\"and 576 image tokens = {576 + num_text_tokens} tokens/embeddings\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"After merging the image and text embeddings the final shape of the embeddings and attention mask is: \"\n",
    "    f\"{tuple(new_inputs_embeds.shape)} - {tuple(new_attention_mask.shape)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompts: \"['<image>\\\\nDescribe the image.', '<image>\\\\nCount the cats.']\"\n",
      "Tokenizer input_ids: tensor([[     2, 256000,    108,  50721,    573,   2416, 235265],\n",
      "        [     2, 256000,    108,   3074,    573,  19493, 235265]])\n",
      "Image features shape: (2, 576, 2048)\n",
      "Each prompt will have 6 text tokens and 576 image tokens = 582 tokens/embeddings\n",
      "After merging the image and text embeddings the final shape of the embeddings and attention mask is: (2, 582, 2048) - (2, 582)\n"
     ]
    }
   ],
   "source": [
    "# With a batch of prompts\n",
    "prompts = [f\"{image_placeholder_token}\\nDescribe the image.\", f\"{image_placeholder_token}\\nCount the cats.\"]\n",
    "image_token_id = llm_tokenizer.convert_tokens_to_ids(image_placeholder_token)\n",
    "tokenizer_output = llm_tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "print(f\"Using prompts: {repr(str(prompts))}\")\n",
    "print(f\"Tokenizer input_ids: {tokenizer_output.input_ids}\")\n",
    "\n",
    "# Use the same image twice\n",
    "image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
    "inputs = image_processor(images=[image, image], padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = vision_model_with_adapter(**inputs)\n",
    "\n",
    "print(f\"Image features shape: {tuple(image_features.shape)}\")\n",
    "\n",
    "new_inputs_embeds, new_attention_mask = merge_input_ids_with_image_features(\n",
    "    image_features, tokenizer_output.input_ids, tokenizer_output.attention_mask, llm_model, image_token_id\n",
    ")\n",
    "\n",
    "# We can validate that both images ends up with the same embedding at the same place\n",
    "assert torch.equal(new_inputs_embeds[:, 1:577, :], image_features)  # First element of each sequence is <bos> embedding\n",
    "\n",
    "# Some shape information\n",
    "num_text_tokens = tokenizer_output.input_ids.shape[1] - 1\n",
    "print(f\"Each prompt will have {num_text_tokens} text tokens \"\n",
    "      f\"and 576 image tokens = {576 + num_text_tokens} tokens/embeddings\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"After merging the image and text embeddings the final shape of the embeddings and attention mask is: \"\n",
    "    f\"{tuple(new_inputs_embeds.shape)} - {tuple(new_attention_mask.shape)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create one model with everything we need\n",
    "We want to create a model with each element that is needed to do the inference. This model needs to store the visual encoder (with the adapter) and the language model. In addition, we want to train the model with the HuggingFace Trainer library and store and load its weights so we will create this model as a HuggingFace model.\n",
    "\n",
    "To do inference with a HuggingFace model we use the `generate` method. To use this method on a custom HuggingFace model we need to implement `prepare_inputs_for_generation` and `_reorder_cache` due to the peculiarities of our model  (we need to get the image embeddings from the vision encoder). But we can do it more easily by relying on the language model to do the generation part. We just need to combine the text and image embeds before calling generate on the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1cfab58a1640b6b7b95c206eff5a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prompt: '<start_of_turn>user\\n<image>\\nDescribe the image.<end_of_turn>\\n<start_of_turn>model\\n'\n",
      "Tokenizer input_ids: tensor([[     2,    106,   1645,    108, 256000,    108,  50721,    573,   2416,\n",
      "         235265,    107,    108,    106,   2516,    108]])\n",
      "Image shape: (1, 3, 384, 384)\n",
      "Raw model's output (id of the tokens): tensor([[235248,    108, 235248,    108, 235248,    108, 235248,    108, 235295,\n",
      "         235278,    483, 235303, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000,  16590,    675,   3591,  11615,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 235269,    665,   1412, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000,    139,    559, 256000, 236666, 236666, 235269,    665, 127829,\n",
      "         256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000, 256000,\n",
      "         256000, 256000, 256000, 256000, 236666, 256000, 236666, 256000, 236666,\n",
      "         256000, 236666, 256000,   3431, 235303, 256000, 236666, 256000, 236666,\n",
      "         256000, 236666, 256000,  14305]])\n",
      "Final output without special tokens: \" \\n \\n \\n \\nP(or' dealing with proper technique, it should  of일일, itasjoner일일일일 об'일일일TM\"\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.modeling_utils import ModelOutput\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ModifiedSiglipVisionModelWithAdapter(nn.Module):\n",
    "    def __init__(self, ref_clip_vision_model: nn.Module, llm_model:AutoModel):\n",
    "        super().__init__()\n",
    "        self.config = ref_clip_vision_model.config\n",
    "\n",
    "        self.embeddings = ref_clip_vision_model.embeddings\n",
    "        self.encoder = ref_clip_vision_model.encoder\n",
    "\n",
    "        self.adapter = nn.Linear(in_features=self.config.hidden_size, out_features=llm_model.config.hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "\n",
    "        # Convert pixel values to embeddings\n",
    "        embeddings = self.embeddings(pixel_values)\n",
    "\n",
    "        # Process embeddings through the encoder\n",
    "        encoder_outputs = self.encoder(inputs_embeds=embeddings)\n",
    "\n",
    "        # There is only one output: last_hidden_state\n",
    "        sequence_output = encoder_outputs[0]  # `encoder_outputs.last_hidden_state` is the same\n",
    "\n",
    "        # Prepare the output for the LLM\n",
    "        sequence_output = self.adapter(sequence_output)\n",
    "\n",
    "        return sequence_output\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VLMOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class VLMConfig(PretrainedConfig):\n",
    "    model_type = \"vlm\"\n",
    "    is_composition = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "class VLM(PreTrainedModel):\n",
    "    config_class = VLMConfig\n",
    "\n",
    "    def __init__(self, llm_model, clip_vision_model, image_placeholder_token, image_token_id, num_tokens):\n",
    "        super().__init__(VLM.config_class())\n",
    "\n",
    "        self.llm_model = llm_model\n",
    "        self.vision_model_with_adapter = ModifiedSiglipVisionModelWithAdapter(clip_vision_model, llm_model)       \n",
    "\n",
    "        self.image_placeholder_token = image_placeholder_token\n",
    "        self.image_token_id = image_token_id\n",
    "\n",
    "        # Resize the embedding layer to accommodate any additional token\n",
    "        self.llm_model.resize_token_embeddings(num_tokens)\n",
    "\n",
    "        # Adjust configuration of the llm model\n",
    "        self.llm_model.config.vocab_size = num_tokens\n",
    "\n",
    "    def _merge_input_ids_with_image_features(self, image_features, input_ids, attention_mask):\n",
    "        # Initialize variables for the new sequence dimensions\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        image_feature_dim = image_features.size(-1)\n",
    "        image_seq_length = image_features.size(1)\n",
    "\n",
    "        # Convert input_ids to embeddings\n",
    "        inputs_embeds = self.llm_model.model.embed_tokens(input_ids)\n",
    "\n",
    "        # Device adjustment for new tensors based on the device of inputs_embeds\n",
    "        device = inputs_embeds.device\n",
    "        \n",
    "        # Find positions of the <image> tokens\n",
    "        image_token_mask = input_ids == self.image_token_id\n",
    "        num_image_tokens_per_seq = torch.sum(image_token_mask, dim=-1)\n",
    "        image_token_positions = image_token_mask.nonzero()\n",
    "\n",
    "        # There must be always one and only one image per sequence\n",
    "        if not all(num_image_tokens_per_seq == 1):\n",
    "            raise RuntimeError(\n",
    "                f\"Expecting one and only one image (id={self.image_token_id}) per sequence in the batch: {input_ids}\"\n",
    "            )\n",
    "        \n",
    "        # All image tokens must be at the same position\n",
    "        if not all(image_token_positions[0, 1] == image_token_positions[:, 1]):\n",
    "            raise RuntimeError(\n",
    "                f\"Image token (id={self.image_token_id}) is not at the same position in all sequences of the batch:\"\n",
    "                f\"{input_ids}\"\n",
    "            )\n",
    "\n",
    "        # Calculate the new sequence length after inserting image features\n",
    "        new_seq_length =  image_seq_length + seq_length - 1  # - 1 because we remove the image token\n",
    "\n",
    "        # Prepare containers for the new embeddings and attention mask\n",
    "        new_inputs_embeds = torch.zeros(batch_size, new_seq_length, image_feature_dim, device=device)\n",
    "        new_attention_mask = torch.zeros(batch_size, new_seq_length, device=device)\n",
    "        \n",
    "        # Copy the text and image embeddings into the new containers\n",
    "        for batch_idx, pos in image_token_positions:\n",
    "            # Copy text embeddings up to the image token\n",
    "            if pos > 0:\n",
    "                new_inputs_embeds[:, :pos, :] = inputs_embeds[:, :pos, :]\n",
    "                new_attention_mask[:, :pos] = attention_mask[:, :pos]\n",
    "\n",
    "            # Insert image features\n",
    "            image_start_idx = pos\n",
    "            image_end_idx = image_start_idx + image_seq_length\n",
    "            new_inputs_embeds[batch_idx, image_start_idx:image_end_idx, :] = image_features[batch_idx, :, :]\n",
    "            new_attention_mask[batch_idx, image_start_idx:image_end_idx] = 1\n",
    "\n",
    "            # Copy remaining text embeddings after the <image> token\n",
    "            if pos < seq_length - 1:\n",
    "                remaining_text_start_idx = image_end_idx\n",
    "                new_inputs_embeds[batch_idx, remaining_text_start_idx:, :] = inputs_embeds[batch_idx, pos+1:, :]\n",
    "                new_attention_mask[batch_idx, remaining_text_start_idx:] = attention_mask[batch_idx, pos+1:]\n",
    "\n",
    "        return new_inputs_embeds, new_attention_mask\n",
    "\n",
    "    def create_prompt(self, instruction: str, expected_response: str = \"\"):\n",
    "        # This function is what you have to use before sending the text to the tokenizer\n",
    "        # `expected_response` is only for training\n",
    "        return (\n",
    "            f\"<start_of_turn>user\\n{self.image_placeholder_token}\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "            f\"{expected_response}\"\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else False\n",
    "        \n",
    "        # Get the embeddings for the image\n",
    "        image_features = self.vision_model_with_adapter(pixel_values)\n",
    "\n",
    "        # Merge the embeddings of the image and text\n",
    "        inputs_embeds, attention_mask = self._merge_input_ids_with_image_features(\n",
    "            image_features, input_ids, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get the llm model's output\n",
    "        outputs = self.llm_model(\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            if attention_mask is not None:\n",
    "                shift_attention_mask = attention_mask[..., 1:]\n",
    "                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n",
    "                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n",
    "            else:\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return VLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "    def generate(self,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        max_new_tokens: int = 256\n",
    "    ):\n",
    "        # Get the embeddings for the image\n",
    "        image_features = self.vision_model_with_adapter(pixel_values)\n",
    "\n",
    "        # Merge the embeddings of the image and text\n",
    "        inputs_embeds, attention_mask = self._merge_input_ids_with_image_features(\n",
    "            image_features, input_ids, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Generate with the LLM\n",
    "        return self.llm_model.generate(\n",
    "            inputs_embeds=inputs_embeds, attention_mask=attention_mask, max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "\n",
    "# Create each individual component\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(clip_model_folder, local_files_only=True)\n",
    "clip_processor = AutoProcessor.from_pretrained(clip_model_folder, local_files_only=True)\n",
    "\n",
    "# Get the vision model and the image processor from CLIP\n",
    "clip_vision_model = clip_model.vision_model\n",
    "image_processor = clip_processor.image_processor \n",
    "\n",
    "# Prepare the tokenizer\n",
    "image_placeholder_token = '<image>'\n",
    "special_tokens_dict = {'additional_special_tokens': [image_placeholder_token]}\n",
    "num_added_toks = llm_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "image_token_id = llm_tokenizer.convert_tokens_to_ids(image_placeholder_token)\n",
    "num_tokens = len(llm_tokenizer)\n",
    "\n",
    "# Create the model\n",
    "vlm = VLM(llm_model, clip_vision_model, image_placeholder_token, image_token_id, num_tokens)\n",
    "\n",
    "# Prepare the inputs\n",
    "prompt = vlm.create_prompt(instruction=\"Describe the image.\")\n",
    "text_inputs = llm_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "print(f\"Using prompt: {repr(prompt)}\")\n",
    "print(f\"Tokenizer input_ids: {text_inputs.input_ids}\")\n",
    "\n",
    "image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
    "image_inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Image shape: {tuple(image_inputs.pixel_values.shape)}\")\n",
    "\n",
    "# Do inference\n",
    "with torch.no_grad():\n",
    "    generate_ids = vlm.generate(\n",
    "        pixel_values=image_inputs.pixel_values, \n",
    "        input_ids=text_inputs.input_ids, \n",
    "        attention_mask=text_inputs.attention_mask,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "\n",
    "text_output_without_special_tokens = llm_tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "print(f\"Raw model's output (id of the tokens): {generate_ids}\")\n",
    "print(f\"Final output without special tokens: {repr(text_output_without_special_tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
