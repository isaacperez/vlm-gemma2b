{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a VLM\n",
    "To develop a __Visual Language Model__ (VLM), it's essential to handle both textual and visual inputs. For text, we utilize a Large Language Model (LLM), while images are processed through the CLIP model. It's important to note, though, that CLIP includes a minor language component that is superfluous for our needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for LLM\n",
    "llm_model_id = \"google/gemma-2b-it\"\n",
    "llm_model_folder = f\"./models/{llm_model_id.split('/')[-1]}\"\n",
    "\n",
    "# Global constants for CLIP model\n",
    "clip_model_id = \"google/siglip-base-patch16-384\"\n",
    "clip_model_folder = f\"./models/{clip_model_id.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06072179b9bc4381a227a0b727fb0aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_folder, local_files_only=True)\n",
    "\n",
    "clip_model = AutoModel.from_pretrained(clip_model_folder, local_files_only=True)\n",
    "clip_processor = AutoProcessor.from_pretrained(clip_model_folder, local_files_only=True)\n",
    "\n",
    "# Get the vision model and the image processor from CLIP\n",
    "clip_vision_model = clip_model.vision_model\n",
    "image_processor = clip_processor.image_processor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the components\n",
    "\n",
    "CLIP typically generates a singular embedding to represent an entire image. This approach is less desirable for our purposes, as we favor maintaining a sequence of image embeddings. This sequence-based approach preserves more detailed information within the image, as opposed to condensing it into a single, less descriptive vector.\n",
    "\n",
    "In the CLIP model, an image goes through this whole process:\n",
    "\n",
    "1. __Preprocessing of the Image__:\n",
    "  - The input image is first resized to the specified `image_size` of 384x384 pixels.\n",
    "    Itâ€™s then divided into patches of `patch_size` 16x16 pixels. This results in a grid of patches (24x24 = 576 patches for a 384x384 image), as the stride matches the patch size, ensuring no overlap.\n",
    "\n",
    "2. __Patch Embedding__:\n",
    "  - Each patch is flattened and passed through a convolutional layer (`Conv2d`) that acts as a patch embedding layer. This converts each patch into a 768-dimensional vector (`hidden_size` = 768). This step transforms the spatial patch information into a format suitable for processing by the subsequent transformer layers.\n",
    "\n",
    "3. __Position Embedding__:\n",
    "  - Positional embeddings are added to the patch embeddings to retain information about the original position of each patch within the image. The `position_embedding` component embeds the sequential position of each patch into its representation.\n",
    "\n",
    "4. __SiglipEncoder Processing__:\n",
    "  - The resulting embedded patches, now augmented with positional information, are passed through the `SiglipEncoder`. This encoder consists of multiple (`num_hidden_layers` = 12) identical layers, each comprising:\n",
    "    - __Self-Attention Mechanism__: Each layer contains a `SiglipAttention` module that computes self-attention for each patch embedding, allowing the model to weigh the importance of different patches based on their content and relation to other patches.\n",
    "    - __Intermediate Feed-Forward Network (MLP)__: After attention computation, the data passes through a two-layer feed-forward network (MLP) with a GELU-Tanh activation function (`hidden_act = \"gelu_pytorch_tanh\"`) between the layers, which allows for non-linear processing of each patch's information.\n",
    "    - __Layer Normalization__: Each attention and MLP operation is followed by layer normalization (`layer_norm_eps = 1e-06`) to stabilize the learning process and improve convergence.\n",
    "\n",
    "5. __Post Layer Normalization__:\n",
    "  - After processing through the encoder, a final layer normalization step is applied to the output of the last encoder layer to ensure that the output features are normalized before passing to the classification head.\n",
    "\n",
    "6. __Classification Head__:\n",
    "  - The `SiglipMultiheadAttentionPoolingHead` combines the features from all patches to form a single coherent representation of the image. This involves another round of attention to pool information across patches, followed by layer normalization and a final MLP for processing.\n",
    "\n",
    "\n",
    "By the time the input data has passed through all the encoder layers, it has been transformed into a sequence of embeddings. Each embedding in this sequence corresponds to a patch of the original image, but now represents a deeply processed, high-dimensional feature vector encapsulating both the intrinsic properties of the patch and its contextual relationships with other patches in the image. \n",
    "\n",
    "At stage 5, we're left with a series of embedding vectors, each 768 dimensions in size, that collectively depict the processed image. CLIP, however, simplifies this array into a single embedding vector for subsequent comparison with text embeddings. This simplification occurs in stage 6, where the model aggregates the detailed sequence of image embeddings into one cohesive vector that captures the core attributes of the image. An attention layer along with a Multilayer Perceptron (MLP) is employed to forge this final image representation. From the resultant sequence, only the initial vector is retained, serving as the all-encompassing embedding for the image. \n",
    "\n",
    "To keep the whole sequence of embedding, HuggingFace allows us to access the intermediate outputs of the model so that we can retrieve the output we want. You only have to pass `output_hidden_states=True` when you call the model. It will return the hidden states of the model at the output of each layer plus the initial embedding outputs.\n",
    "\n",
    "The problem is that you don't know where they come from. To know which layer corresponds to which output you have to look at the model implementation in HuggingFace and trace `output_hidden_states` through all the submodules to find out which ones are used. For SigLIP the code is [here](https://github.com/huggingface/transformers/blob/df1542581ee89107eea0569ee044fa8797b66ab0/src/transformers/models/siglip/modeling_siglip.py). It uses 13 layers: input embeddings + 12 layers from `SiglipEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model output has: ['last_hidden_state', 'pooler_output', 'hidden_states']\n",
      "We have the output of 13 layers\n",
      "Output shape of layer 1: (1, 576, 768)\n",
      "Output shape of layer 2: (1, 576, 768)\n",
      "Output shape of layer 3: (1, 576, 768)\n",
      "Output shape of layer 4: (1, 576, 768)\n",
      "Output shape of layer 5: (1, 576, 768)\n",
      "Output shape of layer 6: (1, 576, 768)\n",
      "Output shape of layer 7: (1, 576, 768)\n",
      "Output shape of layer 8: (1, 576, 768)\n",
      "Output shape of layer 9: (1, 576, 768)\n",
      "Output shape of layer 10: (1, 576, 768)\n",
      "Output shape of layer 11: (1, 576, 768)\n",
      "Output shape of layer 12: (1, 576, 768)\n",
      "Output shape of layer 13: (1, 576, 768)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Return the intermediate outputs with output_hidden_states=True\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    clip_vision_model_output = clip_vision_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "print(f\"Vision model output has: {list(clip_vision_model_output.keys())}\")\n",
    "\n",
    "# The intermediate outputs are stored at hidden_states\n",
    "hidden_states = clip_vision_model_output.hidden_states\n",
    "print(f\"We have the output of {len(hidden_states)} layers\")\n",
    "\n",
    "# We have one output for the input embedding and one for each layer of the SiglipEncoder. The output shape is always\n",
    "# (batch size, num. patches, embedding size). We use patches of 16x16 pixels for an image of 384x384 => 576 patches\n",
    "for idx, hidden_output in enumerate(hidden_states):\n",
    "    print(f\"Output shape of layer {idx + 1}: {tuple(hidden_output.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with using `output_hidden_states=True` is that it requires too much memory to store all the hidden states. If we only want to get one of them, we can create a new model that only stores the output we need. Also, all the computation that is done after that layer is unnecessary, so we can remove these modules to reduce the memory used by the model and the computations it has to perform.\n",
    "\n",
    "We can inspect the implementation of the vision encoder of SigLIP (`SiglipVisionTransformer`):\n",
    "```python\n",
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = SiglipVisionEmbeddings(config)\n",
    "        self.encoder = SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "        self.head = SiglipMultiheadAttentionPoolingHead(config)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(SIGLIP_VISION_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=SiglipVisionConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "\n",
    "        pooled_output = self.head(last_hidden_state)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "```\n",
    "\n",
    "As we can see it is just a Pytorch Module. It consists of four components:\n",
    "- `embedding`: This is the part of the model responsible for converting raw pixel values of the input images into a higher-dimensional vector space.\n",
    "- `encoder`: The encoder is the core of the model. It processes the sequence of embedded patches through several layers of self-attention and feed-forward neural networks.\n",
    "- `post_layernorm`: it is used to apply normalization after the encoder to ensure that the output across different patches is normalized before any further processing.\n",
    "- `head`: the \"head\" refers to the component that takes the output of the transformer encoder and performs a specific task, such as classification. In the context of `SiglipVisionTransformer`, this head is designed for pooling the transformer outputs into a single vector representation of the input image.\n",
    "\n",
    "If we want the last layer of the encoder, we can get rid of the `post_layernorm` and the `head` and create a model that does the forward pass with the `embedding` and the `encoder` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model output shape: (1, 576, 768)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class ModifiedSiglipVisionModel(nn.Module):\n",
    "    def __init__(self, ref_clip_vision_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.config = ref_clip_vision_model.config\n",
    "\n",
    "        self.embeddings = ref_clip_vision_model.embeddings\n",
    "        self.encoder = ref_clip_vision_model.encoder\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "\n",
    "        # Convert pixel values to embeddings\n",
    "        embeddings = self.embeddings(pixel_values)\n",
    "\n",
    "        # Process embeddings through the encoder\n",
    "        encoder_outputs = self.encoder(inputs_embeds=embeddings)\n",
    "\n",
    "        # There is only one output: last_hidden_state\n",
    "        sequence_output = encoder_outputs[0]  # `encoder_outputs.last_hidden_state` is the same\n",
    "\n",
    "        return sequence_output\n",
    "\n",
    "\n",
    "# Create the vision model\n",
    "vision_model = ModifiedSiglipVisionModel(ref_clip_vision_model=clip_vision_model)\n",
    "\n",
    "# Do inference with the new model\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = image_processor(images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    vision_model_output = vision_model(**inputs)\n",
    "\n",
    "# It's the same output as before\n",
    "assert torch.equal(vision_model_output, hidden_states[-1])\n",
    "print(f\"Vision model output shape: {tuple(vision_model_output.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the configuration and architecture of the model. __Note that the configuration does not correspond to this new module but to the original one__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model configuration:\n",
      "SiglipVisionConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"model_type\": \"siglip_vision_model\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"transformers_version\": \"4.38.2\"\n",
      "}\n",
      "\n",
      "Model architecture:\n",
      "ModifiedSiglipVisionModel(\n",
      "  (embeddings): SiglipVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
      "    (position_embedding): Embedding(576, 768)\n",
      "  )\n",
      "  (encoder): SiglipEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x SiglipEncoderLayer(\n",
      "        (self_attn): SiglipAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SiglipMLP(\n",
      "          (activation_fn): PytorchGELUTanh()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "print(f\"Vision model configuration:\\n{vision_model.config}\")\n",
    "\n",
    "# Model architecture\n",
    "print(f\"Model architecture:\\n{vision_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the number of parameters before and after the prunning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before prunning: 93176064\n",
      "Parameters after prunning: 86087424\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parameters before prunning: {sum(p.numel() for p in clip_vision_model.parameters())}\")\n",
    "print(f\"Parameters after prunning: {sum(p.numel() for p in vision_model.parameters())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
